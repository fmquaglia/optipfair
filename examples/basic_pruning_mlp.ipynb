{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4",
      "authorship_tag": "ABX9TyP4zLMoDB7rtYNG0sT6rdXW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/peremartra/optipfair/blob/main/examples/basic_pruning_mlp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#OptiPFair Notebook Series – Example: Basic Pruning (MLP)\n",
        "\n",
        "![optiPfair Logo](https://github.com/peremartra/optipfair/blob/main/images/optiPfair.png?raw=true)\n",
        "\n",
        "\n",
        "This notebook demonstrates how to use [OptiPFair](https://github.com/peremartra/optipfair) for structured pruning of transformer models with GLU-based MLP layers.  \n",
        "The example covers both percentage-based and expansion-rate-based pruning strategies.\n",
        "\n",
        "##Recommended Environment\n",
        "\n",
        "- **Platform**: [Google Colab](https://colab.research.google.com)  \n",
        "- **Hardware**: GPU runtime (recommended: T4 or better for 1B–3B models)  \n",
        "- **Dependencies**: Installed automatically in the first cell (optipfair, transformers, torch)\n",
        "\n",
        "##by Pere Martra.\n",
        "\n",
        "- [LinkedIn](https://www.linkedin.com/in/pere-martra/?originalSubdomain=es)  \n",
        "- [GitHub](https://github.com/peremartra)  \n",
        "- [X / Twitter](https://x.com/peremartra)\n",
        "\n",
        "---\n",
        "\n",
        "> If you find this useful, please ⭐ the [repository](https://github.com/peremartra/optipfair) and share it!\n",
        "---\n",
        "If you want your favorite LLM to create code with optiPfair, you just need to provide it with the file: [**optipfair_llm_reference_manual.txt**](https://github.com/peremartra/optipfair/blob/main/optipfair_llm_reference_manual.txt), which contains all the necessary information for the LLM to become an expert in using the library.\n"
      ],
      "metadata": {
        "id": "KGNw3Ii67Eyk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Basic OptiPFair Pruning Example\n",
        "\n",
        "This notebook demonstrates how to use OptiPFair for structured pruning of language models.\n",
        "OptiPFair focuses on pruning MLP layers with GLU (Gated Linear Unit) architecture,\n",
        "which is commonly found in modern models like LLaMA, Gemma, Mistral, and others.\n",
        "\n",
        "Author: Pere Martra\n",
        "\n",
        "Designed for Google Colab - GPU runtime recommended"
      ],
      "metadata": {
        "id": "0KUFxK2p_VCn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Installation and Setup\n"
      ],
      "metadata": {
        "id": "Jz5pIP1TGqy5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q transformers optipfair torch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xmcJAJoF95Ho",
        "outputId": "8dc0533f-65ed-4f21-f90c-a96bea98fb1e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m121.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m86.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m47.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m39.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m108.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Import Libraries and Check GPU\n"
      ],
      "metadata": {
        "id": "7c7351uIA7mt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import os\n",
        "import gc\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from optipfair import prune_model\n",
        "\n",
        "# Check device availability\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EUfbDb8e9-0e",
        "outputId": "9671f8eb-ba70-4b98-a7dd-63658f941ef0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "GPU: NVIDIA L4\n",
            "GPU Memory: 22.2 GB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Configuration\n"
      ],
      "metadata": {
        "id": "kua1O8A5BDBX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zps0Yh_m9247",
        "outputId": "5dd2169b-87d9-4039-db20-cbecfa339a1a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Configuration set successfully!\n",
            "Models to test: ['meta-llama/Llama-3.2-1B']\n",
            "Pruning percentage: 20%\n",
            "Target expansion rate: 200%\n"
          ]
        }
      ],
      "source": [
        "# List of models to test - you can add more GLU-compatible models here\n",
        "# Note: For Colab, stick to smaller models due to memory constraints\n",
        "MODELS_TO_TEST = [\n",
        "    \"meta-llama/Llama-3.2-1B\",\n",
        "    # \"google/gemma-2-2b\",  # Uncomment if you have enough GPU memory\n",
        "    # Add more models here as needed\n",
        "]\n",
        "\n",
        "# Pruning configuration - modify these values as needed\n",
        "PRUNING_PERCENTAGE = 20  # Percentage of neurons to remove (0-100)\n",
        "TARGET_EXPANSION_RATE = 200  # Alternative: target expansion rate (e.g., 200% instead of ~400%)\n",
        "\n",
        "# Test prompts for evaluation\n",
        "TEST_PROMPTS = [\n",
        "    \"Paris is the capital of\",\n",
        "    \"The theory of relativity states that\",\n",
        "    \"Machine learning is a field of\",\n",
        "]\n",
        "\n",
        "print(\"Configuration set successfully!\")\n",
        "print(f\"Models to test: {MODELS_TO_TEST}\")\n",
        "print(f\"Pruning percentage: {PRUNING_PERCENTAGE}%\")\n",
        "print(f\"Target expansion rate: {TARGET_EXPANSION_RATE}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction to Structured Pruning\n",
        "\n",
        "---\n",
        "This example demonstrates structured pruning of MLP layers in transformer models.\n",
        "\n",
        "Structured pruning removes entire neurons while maintaining model architecture, resulting in actual speedup and memory reduction during inference.\n"
      ],
      "metadata": {
        "id": "6gTewGHQBNly"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Utility Functions\n"
      ],
      "metadata": {
        "id": "IUXObFzWBucb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def count_parameters(model):\n",
        "    \"\"\"Count total parameters in model\"\"\"\n",
        "    return sum(p.numel() for p in model.parameters())\n",
        "\n",
        "def test_model_generation(model, tokenizer, prompt, max_length=50):\n",
        "    \"\"\"Test text generation with the model\"\"\"\n",
        "    inputs = tokenizer(prompt, return_tensors='pt').to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            inputs['input_ids'],\n",
        "            attention_mask=inputs['attention_mask'],\n",
        "            max_length=max_length,\n",
        "            num_return_sequences=1,\n",
        "            pad_token_id=tokenizer.pad_token_id,\n",
        "            do_sample=False,\n",
        "            num_beams=3,\n",
        "            early_stopping=True,\n",
        "            no_repeat_ngram_size=2\n",
        "        )\n",
        "\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "def print_model_info(model, model_name, stage=\"\"):\n",
        "    \"\"\"Print basic model information\"\"\"\n",
        "    param_count = count_parameters(model)\n",
        "    print(f\"{stage} Model: {model_name}\")\n",
        "    print(f\"Parameters: {param_count:,}\")\n",
        "    return param_count\n",
        "\n",
        "def cleanup_memory():\n",
        "    \"\"\"Clean up GPU memory - important for Colab\"\"\"\n",
        "    gc.collect()\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "print(\"Utility functions defined successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V4TV-6hX-Xio",
        "outputId": "2f60d62c-7837-4a92-bd22-2e29d3ff520e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Utility functions defined successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## OptiPFair Parameters Explanation\n",
        "• model: The model to be pruned\n",
        "\n",
        "• pruning_type: Type of pruning (currently only 'MLP_GLU' supported)\n",
        "\n",
        "• neuron_selection_method: Method to calculate neuron importance:\n",
        "  - 'MAW': Maximum Absolute Weight (recommended for most models)\n",
        "  - 'VOW': Variance of Weights (alternative method)\n",
        "  - 'PON': Product of Norms (alternative method)\n",
        "  \n",
        "• pruning_percentage: Percentage of neurons to remove (0-100)\n",
        "\n",
        "• expansion_rate: Alternative to pruning_percentage - target expansion rate\n",
        "\n",
        "• show_progress: Display progress bar during pruning\n",
        "\n",
        "• return_stats: Return detailed statistics about pruning"
      ],
      "metadata": {
        "id": "0skfNriYCRIW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Example 1 - Pruning by Percentage Function\n"
      ],
      "metadata": {
        "id": "F_ct1e6uDCBm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def example_pruning_by_percentage(model, tokenizer, model_name):\n",
        "    \"\"\"Example of pruning by neuron percentage\"\"\"\n",
        "    print(f\"=== Example 1: Pruning {model_name} by {PRUNING_PERCENTAGE}% ===\")\n",
        "\n",
        "    # Get original model info\n",
        "    original_params = print_model_info(model, model_name, \"Original\")\n",
        "\n",
        "    # Test original model\n",
        "    print(\"\\n--- Original Model Generation ---\")\n",
        "    for prompt in TEST_PROMPTS[:2]:  # Test first 2 prompts\n",
        "        generated = test_model_generation(model, tokenizer, prompt)\n",
        "        print(f\"Prompt: '{prompt}'\")\n",
        "        print(f\"Generated: {generated}\")\n",
        "        print()\n",
        "\n",
        "    # Apply pruning by percentage\n",
        "    pruned_model, stats = prune_model(\n",
        "        model=model,\n",
        "        pruning_type=\"MLP_GLU\",\n",
        "        neuron_selection_method=\"MAW\",  # Change to \"VOW\" or \"PON\" to try other methods\n",
        "        pruning_percentage=PRUNING_PERCENTAGE,\n",
        "        show_progress=True,\n",
        "        return_stats=True\n",
        "    )\n",
        "\n",
        "    # Print pruning statistics\n",
        "    print(\"\\n--- Pruning Results ---\")\n",
        "    print(f\"Original parameters: {stats['original_parameters']:,}\")\n",
        "    print(f\"Pruned parameters: {stats['pruned_parameters']:,}\")\n",
        "    print(f\"Reduction: {stats['reduction']:,} parameters ({stats['percentage_reduction']:.2f}%)\")\n",
        "    print(f\"Final expansion rate: {stats['expansion_rate']:.2f}%\")\n",
        "\n",
        "    # Test pruned model\n",
        "    print(\"\\n--- Pruned Model Generation ---\")\n",
        "    for prompt in TEST_PROMPTS[:2]:\n",
        "        generated = test_model_generation(pruned_model, tokenizer, prompt)\n",
        "        print(f\"Prompt: '{prompt}'\")\n",
        "        print(f\"Generated: {generated}\")\n",
        "        print()\n",
        "\n",
        "    return pruned_model, stats\n",
        "\n",
        "print(\"Example 1 function defined!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "avfKuN66-e76",
        "outputId": "2e469d34-dd7b-43c6-ca40-33b4c8d34cf1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Example 1 function defined!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Example 2 - Pruning by Expansion Rate Function\n"
      ],
      "metadata": {
        "id": "k6b0TlOqEPvH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def example_pruning_by_expansion_rate(model, tokenizer, model_name):\n",
        "    \"\"\"Example of pruning by target expansion rate\"\"\"\n",
        "    print(f\"=== Example 2: Pruning {model_name} to {TARGET_EXPANSION_RATE}% expansion rate ===\")\n",
        "\n",
        "    # Get original model info\n",
        "    original_params = print_model_info(model, model_name, \"Original\")\n",
        "\n",
        "    # Apply pruning by expansion rate\n",
        "    pruned_model, stats = prune_model(\n",
        "        model=model,\n",
        "        pruning_type=\"MLP_GLU\",\n",
        "        neuron_selection_method=\"MAW\",\n",
        "        pruning_percentage=None,  # Must be None when using expansion_rate\n",
        "        expansion_rate=TARGET_EXPANSION_RATE,  # Target expansion rate instead of percentage\n",
        "        show_progress=True,\n",
        "        return_stats=True\n",
        "    )\n",
        "\n",
        "    # Print pruning statistics\n",
        "    print(\"\\n--- Pruning Results ---\")\n",
        "    print(f\"Original parameters: {stats['original_parameters']:,}\")\n",
        "    print(f\"Pruned parameters: {stats['pruned_parameters']:,}\")\n",
        "    print(f\"Reduction: {stats['reduction']:,} parameters ({stats['percentage_reduction']:.2f}%)\")\n",
        "    print(f\"Target expansion rate: {TARGET_EXPANSION_RATE}%\")\n",
        "    print(f\"Actual expansion rate: {stats['expansion_rate']:.2f}%\")\n",
        "\n",
        "    # Test pruned model with one prompt\n",
        "    print(\"\\n--- Pruned Model Generation ---\")\n",
        "    prompt = TEST_PROMPTS[0]\n",
        "    generated = test_model_generation(pruned_model, tokenizer, prompt)\n",
        "    print(f\"Prompt: '{prompt}'\")\n",
        "    print(f\"Generated: {generated}\")\n",
        "    print()\n",
        "\n",
        "    return pruned_model, stats\n",
        "\n",
        "print(\"Example 2 function defined!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RlImNjJh-mir",
        "outputId": "fb327b5f-001c-4416-df77-74f427271ebb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Example 2 function defined!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run Example 1 - Pruning by Percentage\n"
      ],
      "metadata": {
        "id": "gYHavcdMF9Cp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Starting Example 1: Pruning by Percentage\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Process the first model in the list\n",
        "model_name = MODELS_TO_TEST[0]\n",
        "print(f\"Loading model: {model_name}\")\n",
        "\n",
        "# Load model and tokenizer\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    torch_dtype=torch.float16 if device.type == 'cuda' else torch.float32,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Set pad token if not present\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Run Example 1\n",
        "pruned_model_1, stats_1 = example_pruning_by_percentage(model, tokenizer, model_name)\n",
        "\n",
        "# Store stats for summary\n",
        "results = [{\n",
        "    'model': model_name,\n",
        "    'method': 'Percentage',\n",
        "    'reduction': stats_1['percentage_reduction'],\n",
        "    'expansion_rate': stats_1['expansion_rate']\n",
        "}]\n",
        "\n",
        "print(f\"\\nExample 1 completed! Reduction: {stats_1['percentage_reduction']:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uNc30L_O-SBu",
        "outputId": "2ac5fa79-056d-4c8f-8bed-682175d26144"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Example 1: Pruning by Percentage\n",
            "==================================================\n",
            "Loading model: meta-llama/Llama-3.2-1B\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Example 1: Pruning meta-llama/Llama-3.2-1B by 20% ===\n",
            "Original Model: meta-llama/Llama-3.2-1B\n",
            "Parameters: 1,235,814,400\n",
            "\n",
            "--- Original Model Generation ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt: 'Paris is the capital of'\n",
            "Generated: Paris is the capital of France and the largest city in the country. It is located on the River Seine and is one of the most popular tourist destinations in Europe. The city has a population of over 2.2 million people, making\n",
            "\n",
            "Prompt: 'The theory of relativity states that'\n",
            "Generated: The theory of relativity states that the speed of light is the same in all inertial frames of reference. In other words, light always travels at a constant speed, regardless of the motion of its source. This means that if you are moving\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Pruning layers: 100%|██████████| 16/16 [00:05<00:00,  2.81it/s]\n",
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Pruning Results ---\n",
            "Original parameters: 1,235,814,400\n",
            "Pruned parameters: 1,074,792,448\n",
            "Reduction: 161,021,952 parameters (13.03%)\n",
            "Final expansion rate: 320.02%\n",
            "\n",
            "--- Pruned Model Generation ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt: 'Paris is the capital of'\n",
            "Generated: Paris is the capital of France. It is also known as the City of Lights, because it is considered to be one of the most beautiful cities in the world. Paris is famous for its architecture, its museums, and its restaurants. There are\n",
            "\n",
            "Prompt: 'The theory of relativity states that'\n",
            "Generated: The theory of relativity states that there is no such thing as absolute simultaneity, that is to say that two objects moving at the same speed do not necessarily have to be in the exact same time frame. In other words, they do\n",
            "\n",
            "\n",
            "Example 1 completed! Reduction: 13.03%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run Example 2 - Pruning by Expansion Rate\n"
      ],
      "metadata": {
        "id": "8e44y9BzGHZz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Starting Example 2: Pruning by Expansion Rate\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Clean up memory from previous example\n",
        "#del pruned_model_1\n",
        "cleanup_memory()\n",
        "\n",
        "# Reload model for second example (since first one was modified)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    torch_dtype=torch.float16 if device.type == 'cuda' else torch.float32,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "# Run Example 2\n",
        "pruned_model_2, stats_2 = example_pruning_by_expansion_rate(model, tokenizer, model_name)\n",
        "\n",
        "# Add to results\n",
        "results.append({\n",
        "    'model': model_name,\n",
        "    'method': 'Expansion Rate',\n",
        "    'reduction': stats_2['percentage_reduction'],\n",
        "    'expansion_rate': stats_2['expansion_rate']\n",
        "})\n",
        "\n",
        "print(f\"\\nExample 2 completed! Reduction: {stats_2['percentage_reduction']:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3i2qbo9h-sGh",
        "outputId": "c1214691-69fb-4047-ae11-ee6c65d43926"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Example 2: Pruning by Expansion Rate\n",
            "==================================================\n",
            "=== Example 2: Pruning meta-llama/Llama-3.2-1B to 200% expansion rate ===\n",
            "Original Model: meta-llama/Llama-3.2-1B\n",
            "Parameters: 1,235,814,400\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Pruning layers: 100%|██████████| 16/16 [00:03<00:00,  4.49it/s]\n",
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Pruning Results ---\n",
            "Original parameters: 1,235,814,400\n",
            "Pruned parameters: 833,161,216\n",
            "Reduction: 402,653,184 parameters (32.58%)\n",
            "Target expansion rate: 200%\n",
            "Actual expansion rate: 200.00%\n",
            "\n",
            "--- Pruned Model Generation ---\n",
            "Prompt: 'Paris is the capital of'\n",
            "Generated: Paris is the capital of France. It’s the biggest city in the country of the region of Laigneueu. There’s been a long history of time in this city, that it’s one’s most famous. That it is,\n",
            "\n",
            "\n",
            "Example 2 completed! Reduction: 32.58%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Results Summary"
      ],
      "metadata": {
        "id": "JlrGeo5JGQG3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"PRUNING RESULTS SUMMARY\")\n",
        "print(\"=\"*60)\n",
        "print(f\"{'Model':<30} {'Method':<15} {'Reduction':<12} {'Expansion Rate':<15}\")\n",
        "print(\"-\" * 75)\n",
        "\n",
        "for result in results:\n",
        "    print(f\"{result['model']:<30} {result['method']:<15} {result['reduction']:<12.2f}% {result['expansion_rate']:<15.2f}%\")\n",
        "\n",
        "print(f\"\\nTotal models tested: {len(results)}\")\n",
        "print(\"Pruning examples completed successfully!\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rYOb3_Ug-9Nw",
        "outputId": "82fe684a-e199-4bc4-dada-87837049f2f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "PRUNING RESULTS SUMMARY\n",
            "============================================================\n",
            "Model                          Method          Reduction    Expansion Rate \n",
            "---------------------------------------------------------------------------\n",
            "meta-llama/Llama-3.2-1B        Percentage      13.03       % 320.02         %\n",
            "meta-llama/Llama-3.2-1B        Expansion Rate  32.58       % 200.00         %\n",
            "\n",
            "Total models tested: 2\n",
            "Pruning examples completed successfully!\n"
          ]
        }
      ]
    }
  ]
}