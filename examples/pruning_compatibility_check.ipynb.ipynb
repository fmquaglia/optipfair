{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMj6le+zPAOMCP7EqvzlNOL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "de60b68c3bcd464d9d35949c0565b8b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6c431b12abd34865bc2ce3682154f208",
              "IPY_MODEL_4ed23b61c44f4b18ac00fc331bca2fa4",
              "IPY_MODEL_cddb8992ca9f4062af7efa1b7bc110f7"
            ],
            "layout": "IPY_MODEL_bf0b927a48184dacafb11df3ea002865"
          }
        },
        "6c431b12abd34865bc2ce3682154f208": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_478b7aa8887646d6a7fbe61dedaea042",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_c004fe4e450545c193ab24f59f448fb6",
            "value": "config.json:‚Äá100%"
          }
        },
        "4ed23b61c44f4b18ac00fc331bca2fa4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_373f330a0ab1486d9b5689efb4cadc99",
            "max": 880,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c79e3604af164c09b9c9689887f65220",
            "value": 880
          }
        },
        "cddb8992ca9f4062af7efa1b7bc110f7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5923bc0488bd42bc8feee602cbd29fae",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_860f5f21db474dd7b7d51dfd9f1d53b7",
            "value": "‚Äá880/880‚Äá[00:00&lt;00:00,‚Äá74.5kB/s]"
          }
        },
        "bf0b927a48184dacafb11df3ea002865": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "478b7aa8887646d6a7fbe61dedaea042": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c004fe4e450545c193ab24f59f448fb6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "373f330a0ab1486d9b5689efb4cadc99": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c79e3604af164c09b9c9689887f65220": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5923bc0488bd42bc8feee602cbd29fae": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "860f5f21db474dd7b7d51dfd9f1d53b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "aa03b7c6b99644ecb547e819d1f5385b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_398f278840b54fabbb04a2b433ec81b0",
              "IPY_MODEL_2675910cd3b6493ca569670d44eb19f3",
              "IPY_MODEL_bfd248f05ea2416bb81f0496f7ed0490"
            ],
            "layout": "IPY_MODEL_3d6a3bd430fd4baf9fb4d3d3bef4b61c"
          }
        },
        "398f278840b54fabbb04a2b433ec81b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3741109cfd8e4854936301131c67d361",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_13520a2bed5440b8bb1bcc3103d77502",
            "value": "model.safetensors:‚Äá100%"
          }
        },
        "2675910cd3b6493ca569670d44eb19f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a25c1c21277a4e5ea27cf6cea1019650",
            "max": 1999811208,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4951ef5b6fd54508bdd7743734744ae6",
            "value": 1999811208
          }
        },
        "bfd248f05ea2416bb81f0496f7ed0490": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_890c85ddcb9a483f9842df115825be35",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_2a85775c0ef64ae3b6f601c33a4b7704",
            "value": "‚Äá2.00G/2.00G‚Äá[00:31&lt;00:00,‚Äá60.5MB/s]"
          }
        },
        "3d6a3bd430fd4baf9fb4d3d3bef4b61c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3741109cfd8e4854936301131c67d361": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "13520a2bed5440b8bb1bcc3103d77502": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a25c1c21277a4e5ea27cf6cea1019650": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4951ef5b6fd54508bdd7743734744ae6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "890c85ddcb9a483f9842df115825be35": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2a85775c0ef64ae3b6f601c33a4b7704": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/peremartra/optipfair/blob/main/examples/pruning_compatibility_check.ipynb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#OptiPFair Notebook Series ‚Äì  Pruning Compatibility Checker\n",
        "\n",
        "![optiPfair Logo](https://github.com/peremartra/optipfair/blob/main/images/optiPfair.png?raw=true)\n",
        "\n",
        "\n",
        "Verify if your model is compatible with [OptiPFair](https://github.com/peremartra/optipfair) MLP pruning capabilities for structured pruning of transformer models with GLU-based MLP layers.  \n",
        "\n",
        "This notebook quickly verifies if your transformer model is compatible with OptipFair's **structured pruning** capabilities.\n",
        "\n",
        "**In 30 seconds, you'll know:**\n",
        "- Can I prune this model with OptipFair?\n",
        "- What's the model architecture?\n",
        "- What are the MLP expansion ratios?\n",
        "- Any specific recommendations?\n",
        "\n",
        "**Supported architectures:** Llama, Mistral, Gemma, Qwen, Phi, and other GLU-based models.\n",
        "\n",
        "##Recommended Environment\n",
        "\n",
        "- **Platform**: [Google Colab](https://colab.research.google.com)  \n",
        "- **Hardware**: GPU runtime (recommended: T4 or better for 1B‚Äì3B models)  \n",
        "- **Dependencies**: Installed automatically in the first cell (optipfair, transformers, torch)\n",
        "\n",
        "##by Pere Martra.\n",
        "\n",
        "- [LinkedIn](https://www.linkedin.com/in/pere-martra)  \n",
        "- [GitHub](https://github.com/peremartra)  \n",
        "- [X / Twitter](https://x.com/peremartra)\n"
      ],
      "metadata": {
        "id": "fAJBzfQOTj2J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup"
      ],
      "metadata": {
        "id": "Cc3wzmUGUnSE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install OptipFair if not already installed\n",
        "!pip install transformers torch -q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jBtixpRoUuBL",
        "outputId": "4d13b709-ca85-46e0-b89f-9e0fddd442e0"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m35.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m26.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m39.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m84.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModel, AutoConfig\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"‚úÖ Setup complete!\")\n",
        "print(f\"üî• PyTorch version: {torch.__version__}\")\n",
        "print(f\"ü§ó Device: {'GPU' if torch.cuda.is_available() else 'CPU'}\")\n",
        "print(\"üì¶ OptipFair will be used for actual pruning operations\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5VPvLRvYUzC_",
        "outputId": "6535bd50-24a3-4f0b-df3a-a5a45c38b045"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Setup complete!\n",
            "üî• PyTorch version: 2.6.0+cu124\n",
            "ü§ó Device: GPU\n",
            "üì¶ OptipFair will be used for actual pruning operations\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Input\n",
        "**Enter your model name below:**  \n",
        "You can use any Hugging Face model ID (e.g., `microsoft/Phi-3-mini-4k-instruct`, `google/gemma-2-2b`)\n"
      ],
      "metadata": {
        "id": "p2dRv7SVVtSc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# üëá EDIT THIS: Enter your model name\n",
        "MODEL_NAME = \"google/gemma-3-1b-pt\"  # Change this to test your model\n",
        "#MODEL_NAME = \"meta-llama/Llama-3.2-1B\"  # Change this to test your model\n",
        "#MODEL_NAME = \"Qwen/Qwen3-0.6B\"\n",
        "print(f\"üîç Checking compatibility for: {MODEL_NAME}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2vrWQeLKV0RE",
        "outputId": "5b74bf75-4f0e-415b-eb37-0c1732cbe0de"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîç Checking compatibility for: google/gemma-3-1b-pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Compatibility Analysis"
      ],
      "metadata": {
        "id": "q7GSN8JwV3Xr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def check_model_compatibility(model_name):\n",
        "    \"\"\"\n",
        "    Comprehensive compatibility check for OptipFair pruning\n",
        "    \"\"\"\n",
        "    try:\n",
        "        print(\"üîÑ Loading model configuration...\")\n",
        "        config = AutoConfig.from_pretrained(model_name)\n",
        "\n",
        "        # Initialize results\n",
        "        results = {\n",
        "            \"model_name\": model_name,\n",
        "            \"compatible\": False,\n",
        "            \"architecture\": \"Unknown\",\n",
        "            \"issues\": [],\n",
        "            \"recommendations\": [],\n",
        "            \"details\": {}\n",
        "        }\n",
        "\n",
        "        # Extract basic info with proper handling of missing fields\n",
        "        results[\"details\"][\"model_type\"] = getattr(config, 'model_type', 'Unknown')\n",
        "        results[\"details\"][\"num_layers\"] = getattr(config, 'num_hidden_layers', 'N/A')\n",
        "        results[\"details\"][\"hidden_size\"] = getattr(config, 'hidden_size', 'N/A')\n",
        "        results[\"details\"][\"intermediate_size\"] = getattr(config, 'intermediate_size', 'N/A')\n",
        "\n",
        "        # Calculate expansion ratio from config if possible\n",
        "        hidden_size = getattr(config, 'hidden_size', None)\n",
        "        intermediate_size = getattr(config, 'intermediate_size', None)\n",
        "\n",
        "        if hidden_size and intermediate_size and hidden_size > 0:\n",
        "            config_expansion_ratio = (intermediate_size / hidden_size) * 100\n",
        "            results[\"details\"][\"config_expansion_ratio\"] = f\"{config_expansion_ratio:.0f}%\"\n",
        "        else:\n",
        "            results[\"details\"][\"config_expansion_ratio\"] = \"N/A\"\n",
        "\n",
        "        print(f\"üìä Model type: {results['details']['model_type']}\")\n",
        "        print(f\"üìä Layers: {results['details']['num_layers']}\")\n",
        "        print(f\"üìä Hidden size: {results['details']['hidden_size']}\")\n",
        "        print(f\"üìä Intermediate size: {results['details']['intermediate_size']}\")\n",
        "        print(f\"üìä Config expansion ratio: {results['details']['config_expansion_ratio']}\")\n",
        "\n",
        "        return results\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error loading model: {str(e)}\")\n",
        "        return None"
      ],
      "metadata": {
        "id": "FrPnIXMSWbkY"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the check\n",
        "compatibility_results = check_model_compatibility(MODEL_NAME)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153,
          "referenced_widgets": [
            "de60b68c3bcd464d9d35949c0565b8b5",
            "6c431b12abd34865bc2ce3682154f208",
            "4ed23b61c44f4b18ac00fc331bca2fa4",
            "cddb8992ca9f4062af7efa1b7bc110f7",
            "bf0b927a48184dacafb11df3ea002865",
            "478b7aa8887646d6a7fbe61dedaea042",
            "c004fe4e450545c193ab24f59f448fb6",
            "373f330a0ab1486d9b5689efb4cadc99",
            "c79e3604af164c09b9c9689887f65220",
            "5923bc0488bd42bc8feee602cbd29fae",
            "860f5f21db474dd7b7d51dfd9f1d53b7"
          ]
        },
        "id": "RFaqi8NSWgCu",
        "outputId": "23d34176-a41c-4640-91f7-04b41f2dc9ce"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîÑ Loading model configuration...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/880 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "de60b68c3bcd464d9d35949c0565b8b5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìä Model type: gemma3_text\n",
            "üìä Layers: 26\n",
            "üìä Hidden size: 1152\n",
            "üìä Intermediate size: 6912\n",
            "üìä Config expansion ratio: 600%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def analyze_mlp_structure(model_name, config):\n",
        "    \"\"\"\n",
        "    Analyze MLP structure for pruning compatibility\n",
        "    \"\"\"\n",
        "    try:\n",
        "        print(\"\\nüîç Analyzing MLP structure...\")\n",
        "\n",
        "        # Load a small portion of the model to inspect structure\n",
        "        model = AutoModel.from_pretrained(\n",
        "            model_name,\n",
        "            torch_dtype=torch.float16,\n",
        "            device_map=\"auto\" if torch.cuda.is_available() else \"cpu\",\n",
        "            trust_remote_code=True\n",
        "        )\n",
        "\n",
        "        # Try to find the first transformer layer (different models have different structures)\n",
        "        first_layer = None\n",
        "        if hasattr(model, 'layers') and len(model.layers) > 0:\n",
        "            first_layer = model.layers[0]\n",
        "        elif hasattr(model, 'transformer') and hasattr(model.transformer, 'h') and len(model.transformer.h) > 0:\n",
        "            first_layer = model.transformer.h[0]\n",
        "        elif hasattr(model, 'model') and hasattr(model.model, 'layers') and len(model.model.layers) > 0:\n",
        "            first_layer = model.model.layers[0]\n",
        "\n",
        "        if first_layer is None:\n",
        "            print(\"‚ö†Ô∏è  Could not find transformer layers\")\n",
        "            return None\n",
        "\n",
        "        # Try to find MLP/feed_forward layer\n",
        "        mlp = None\n",
        "        if hasattr(first_layer, 'mlp'):\n",
        "            mlp = first_layer.mlp\n",
        "        elif hasattr(first_layer, 'feed_forward'):\n",
        "            mlp = first_layer.feed_forward\n",
        "        elif hasattr(first_layer, 'ffn'):\n",
        "            mlp = first_layer.ffn\n",
        "\n",
        "        if mlp is None:\n",
        "            print(\"‚ö†Ô∏è  Could not find MLP layer\")\n",
        "            return None\n",
        "\n",
        "        # Check for GLU structure (gate_proj + up_proj + down_proj)\n",
        "        has_gate_proj = hasattr(mlp, 'gate_proj')\n",
        "        has_up_proj = hasattr(mlp, 'up_proj')\n",
        "        has_down_proj = hasattr(mlp, 'down_proj')\n",
        "\n",
        "        # Alternative names for some models\n",
        "        if not has_gate_proj:\n",
        "            has_gate_proj = hasattr(mlp, 'w1') or hasattr(mlp, 'gate_linear')\n",
        "        if not has_up_proj:\n",
        "            has_up_proj = hasattr(mlp, 'w3') or hasattr(mlp, 'up_linear')\n",
        "        if not has_down_proj:\n",
        "            has_down_proj = hasattr(mlp, 'w2') or hasattr(mlp, 'down_linear')\n",
        "\n",
        "        # Calculate expansion ratio\n",
        "        expansion_ratio = 0\n",
        "        input_dim = 0\n",
        "        output_dim = 0\n",
        "\n",
        "        try:\n",
        "            if has_gate_proj and has_up_proj:\n",
        "                gate_layer = getattr(mlp, 'gate_proj', getattr(mlp, 'w1', None))\n",
        "                if gate_layer and hasattr(gate_layer, 'in_features') and hasattr(gate_layer, 'out_features'):\n",
        "                    input_dim = gate_layer.in_features\n",
        "                    output_dim = gate_layer.out_features\n",
        "                    expansion_ratio = (output_dim / input_dim) * 100\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è  Could not calculate expansion ratio: {str(e)}\")\n",
        "\n",
        "        # Clean up memory\n",
        "        del model\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "        return {\n",
        "            \"has_glu\": has_gate_proj and has_up_proj and has_down_proj,\n",
        "            \"expansion_ratio\": expansion_ratio,\n",
        "            \"input_dim\": input_dim,\n",
        "            \"output_dim\": output_dim,\n",
        "            \"found_mlp\": True\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è  Could not analyze MLP structure: {str(e)}\")\n",
        "        return {\n",
        "            \"has_glu\": False,\n",
        "            \"expansion_ratio\": 0,\n",
        "            \"input_dim\": 0,\n",
        "            \"output_dim\": 0,\n",
        "            \"found_mlp\": False,\n",
        "            \"error\": str(e)\n",
        "        }\n",
        "\n",
        "# Analyze MLP structure\n",
        "if compatibility_results:\n",
        "    mlp_analysis = analyze_mlp_structure(MODEL_NAME, AutoConfig.from_pretrained(MODEL_NAME))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138,
          "referenced_widgets": [
            "aa03b7c6b99644ecb547e819d1f5385b",
            "398f278840b54fabbb04a2b433ec81b0",
            "2675910cd3b6493ca569670d44eb19f3",
            "bfd248f05ea2416bb81f0496f7ed0490",
            "3d6a3bd430fd4baf9fb4d3d3bef4b61c",
            "3741109cfd8e4854936301131c67d361",
            "13520a2bed5440b8bb1bcc3103d77502",
            "a25c1c21277a4e5ea27cf6cea1019650",
            "4951ef5b6fd54508bdd7743734744ae6",
            "890c85ddcb9a483f9842df115825be35",
            "2a85775c0ef64ae3b6f601c33a4b7704"
          ]
        },
        "id": "XT35T3x5WjIW",
        "outputId": "93958da0-a8d9-4f91-c598-7350bc20c693"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üîç Analyzing MLP structure...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/2.00G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "aa03b7c6b99644ecb547e819d1f5385b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of Gemma3TextModel were not initialized from the model checkpoint at google/gemma-3-1b-pt and are newly initialized: ['embed_tokens.weight', 'layers.0.input_layernorm.weight', 'layers.0.mlp.down_proj.weight', 'layers.0.mlp.gate_proj.weight', 'layers.0.mlp.up_proj.weight', 'layers.0.post_attention_layernorm.weight', 'layers.0.post_feedforward_layernorm.weight', 'layers.0.pre_feedforward_layernorm.weight', 'layers.0.self_attn.k_norm.weight', 'layers.0.self_attn.k_proj.weight', 'layers.0.self_attn.o_proj.weight', 'layers.0.self_attn.q_norm.weight', 'layers.0.self_attn.q_proj.weight', 'layers.0.self_attn.v_proj.weight', 'layers.1.input_layernorm.weight', 'layers.1.mlp.down_proj.weight', 'layers.1.mlp.gate_proj.weight', 'layers.1.mlp.up_proj.weight', 'layers.1.post_attention_layernorm.weight', 'layers.1.post_feedforward_layernorm.weight', 'layers.1.pre_feedforward_layernorm.weight', 'layers.1.self_attn.k_norm.weight', 'layers.1.self_attn.k_proj.weight', 'layers.1.self_attn.o_proj.weight', 'layers.1.self_attn.q_norm.weight', 'layers.1.self_attn.q_proj.weight', 'layers.1.self_attn.v_proj.weight', 'layers.10.input_layernorm.weight', 'layers.10.mlp.down_proj.weight', 'layers.10.mlp.gate_proj.weight', 'layers.10.mlp.up_proj.weight', 'layers.10.post_attention_layernorm.weight', 'layers.10.post_feedforward_layernorm.weight', 'layers.10.pre_feedforward_layernorm.weight', 'layers.10.self_attn.k_norm.weight', 'layers.10.self_attn.k_proj.weight', 'layers.10.self_attn.o_proj.weight', 'layers.10.self_attn.q_norm.weight', 'layers.10.self_attn.q_proj.weight', 'layers.10.self_attn.v_proj.weight', 'layers.11.input_layernorm.weight', 'layers.11.mlp.down_proj.weight', 'layers.11.mlp.gate_proj.weight', 'layers.11.mlp.up_proj.weight', 'layers.11.post_attention_layernorm.weight', 'layers.11.post_feedforward_layernorm.weight', 'layers.11.pre_feedforward_layernorm.weight', 'layers.11.self_attn.k_norm.weight', 'layers.11.self_attn.k_proj.weight', 'layers.11.self_attn.o_proj.weight', 'layers.11.self_attn.q_norm.weight', 'layers.11.self_attn.q_proj.weight', 'layers.11.self_attn.v_proj.weight', 'layers.12.input_layernorm.weight', 'layers.12.mlp.down_proj.weight', 'layers.12.mlp.gate_proj.weight', 'layers.12.mlp.up_proj.weight', 'layers.12.post_attention_layernorm.weight', 'layers.12.post_feedforward_layernorm.weight', 'layers.12.pre_feedforward_layernorm.weight', 'layers.12.self_attn.k_norm.weight', 'layers.12.self_attn.k_proj.weight', 'layers.12.self_attn.o_proj.weight', 'layers.12.self_attn.q_norm.weight', 'layers.12.self_attn.q_proj.weight', 'layers.12.self_attn.v_proj.weight', 'layers.13.input_layernorm.weight', 'layers.13.mlp.down_proj.weight', 'layers.13.mlp.gate_proj.weight', 'layers.13.mlp.up_proj.weight', 'layers.13.post_attention_layernorm.weight', 'layers.13.post_feedforward_layernorm.weight', 'layers.13.pre_feedforward_layernorm.weight', 'layers.13.self_attn.k_norm.weight', 'layers.13.self_attn.k_proj.weight', 'layers.13.self_attn.o_proj.weight', 'layers.13.self_attn.q_norm.weight', 'layers.13.self_attn.q_proj.weight', 'layers.13.self_attn.v_proj.weight', 'layers.14.input_layernorm.weight', 'layers.14.mlp.down_proj.weight', 'layers.14.mlp.gate_proj.weight', 'layers.14.mlp.up_proj.weight', 'layers.14.post_attention_layernorm.weight', 'layers.14.post_feedforward_layernorm.weight', 'layers.14.pre_feedforward_layernorm.weight', 'layers.14.self_attn.k_norm.weight', 'layers.14.self_attn.k_proj.weight', 'layers.14.self_attn.o_proj.weight', 'layers.14.self_attn.q_norm.weight', 'layers.14.self_attn.q_proj.weight', 'layers.14.self_attn.v_proj.weight', 'layers.15.input_layernorm.weight', 'layers.15.mlp.down_proj.weight', 'layers.15.mlp.gate_proj.weight', 'layers.15.mlp.up_proj.weight', 'layers.15.post_attention_layernorm.weight', 'layers.15.post_feedforward_layernorm.weight', 'layers.15.pre_feedforward_layernorm.weight', 'layers.15.self_attn.k_norm.weight', 'layers.15.self_attn.k_proj.weight', 'layers.15.self_attn.o_proj.weight', 'layers.15.self_attn.q_norm.weight', 'layers.15.self_attn.q_proj.weight', 'layers.15.self_attn.v_proj.weight', 'layers.16.input_layernorm.weight', 'layers.16.mlp.down_proj.weight', 'layers.16.mlp.gate_proj.weight', 'layers.16.mlp.up_proj.weight', 'layers.16.post_attention_layernorm.weight', 'layers.16.post_feedforward_layernorm.weight', 'layers.16.pre_feedforward_layernorm.weight', 'layers.16.self_attn.k_norm.weight', 'layers.16.self_attn.k_proj.weight', 'layers.16.self_attn.o_proj.weight', 'layers.16.self_attn.q_norm.weight', 'layers.16.self_attn.q_proj.weight', 'layers.16.self_attn.v_proj.weight', 'layers.17.input_layernorm.weight', 'layers.17.mlp.down_proj.weight', 'layers.17.mlp.gate_proj.weight', 'layers.17.mlp.up_proj.weight', 'layers.17.post_attention_layernorm.weight', 'layers.17.post_feedforward_layernorm.weight', 'layers.17.pre_feedforward_layernorm.weight', 'layers.17.self_attn.k_norm.weight', 'layers.17.self_attn.k_proj.weight', 'layers.17.self_attn.o_proj.weight', 'layers.17.self_attn.q_norm.weight', 'layers.17.self_attn.q_proj.weight', 'layers.17.self_attn.v_proj.weight', 'layers.18.input_layernorm.weight', 'layers.18.mlp.down_proj.weight', 'layers.18.mlp.gate_proj.weight', 'layers.18.mlp.up_proj.weight', 'layers.18.post_attention_layernorm.weight', 'layers.18.post_feedforward_layernorm.weight', 'layers.18.pre_feedforward_layernorm.weight', 'layers.18.self_attn.k_norm.weight', 'layers.18.self_attn.k_proj.weight', 'layers.18.self_attn.o_proj.weight', 'layers.18.self_attn.q_norm.weight', 'layers.18.self_attn.q_proj.weight', 'layers.18.self_attn.v_proj.weight', 'layers.19.input_layernorm.weight', 'layers.19.mlp.down_proj.weight', 'layers.19.mlp.gate_proj.weight', 'layers.19.mlp.up_proj.weight', 'layers.19.post_attention_layernorm.weight', 'layers.19.post_feedforward_layernorm.weight', 'layers.19.pre_feedforward_layernorm.weight', 'layers.19.self_attn.k_norm.weight', 'layers.19.self_attn.k_proj.weight', 'layers.19.self_attn.o_proj.weight', 'layers.19.self_attn.q_norm.weight', 'layers.19.self_attn.q_proj.weight', 'layers.19.self_attn.v_proj.weight', 'layers.2.input_layernorm.weight', 'layers.2.mlp.down_proj.weight', 'layers.2.mlp.gate_proj.weight', 'layers.2.mlp.up_proj.weight', 'layers.2.post_attention_layernorm.weight', 'layers.2.post_feedforward_layernorm.weight', 'layers.2.pre_feedforward_layernorm.weight', 'layers.2.self_attn.k_norm.weight', 'layers.2.self_attn.k_proj.weight', 'layers.2.self_attn.o_proj.weight', 'layers.2.self_attn.q_norm.weight', 'layers.2.self_attn.q_proj.weight', 'layers.2.self_attn.v_proj.weight', 'layers.20.input_layernorm.weight', 'layers.20.mlp.down_proj.weight', 'layers.20.mlp.gate_proj.weight', 'layers.20.mlp.up_proj.weight', 'layers.20.post_attention_layernorm.weight', 'layers.20.post_feedforward_layernorm.weight', 'layers.20.pre_feedforward_layernorm.weight', 'layers.20.self_attn.k_norm.weight', 'layers.20.self_attn.k_proj.weight', 'layers.20.self_attn.o_proj.weight', 'layers.20.self_attn.q_norm.weight', 'layers.20.self_attn.q_proj.weight', 'layers.20.self_attn.v_proj.weight', 'layers.21.input_layernorm.weight', 'layers.21.mlp.down_proj.weight', 'layers.21.mlp.gate_proj.weight', 'layers.21.mlp.up_proj.weight', 'layers.21.post_attention_layernorm.weight', 'layers.21.post_feedforward_layernorm.weight', 'layers.21.pre_feedforward_layernorm.weight', 'layers.21.self_attn.k_norm.weight', 'layers.21.self_attn.k_proj.weight', 'layers.21.self_attn.o_proj.weight', 'layers.21.self_attn.q_norm.weight', 'layers.21.self_attn.q_proj.weight', 'layers.21.self_attn.v_proj.weight', 'layers.22.input_layernorm.weight', 'layers.22.mlp.down_proj.weight', 'layers.22.mlp.gate_proj.weight', 'layers.22.mlp.up_proj.weight', 'layers.22.post_attention_layernorm.weight', 'layers.22.post_feedforward_layernorm.weight', 'layers.22.pre_feedforward_layernorm.weight', 'layers.22.self_attn.k_norm.weight', 'layers.22.self_attn.k_proj.weight', 'layers.22.self_attn.o_proj.weight', 'layers.22.self_attn.q_norm.weight', 'layers.22.self_attn.q_proj.weight', 'layers.22.self_attn.v_proj.weight', 'layers.23.input_layernorm.weight', 'layers.23.mlp.down_proj.weight', 'layers.23.mlp.gate_proj.weight', 'layers.23.mlp.up_proj.weight', 'layers.23.post_attention_layernorm.weight', 'layers.23.post_feedforward_layernorm.weight', 'layers.23.pre_feedforward_layernorm.weight', 'layers.23.self_attn.k_norm.weight', 'layers.23.self_attn.k_proj.weight', 'layers.23.self_attn.o_proj.weight', 'layers.23.self_attn.q_norm.weight', 'layers.23.self_attn.q_proj.weight', 'layers.23.self_attn.v_proj.weight', 'layers.24.input_layernorm.weight', 'layers.24.mlp.down_proj.weight', 'layers.24.mlp.gate_proj.weight', 'layers.24.mlp.up_proj.weight', 'layers.24.post_attention_layernorm.weight', 'layers.24.post_feedforward_layernorm.weight', 'layers.24.pre_feedforward_layernorm.weight', 'layers.24.self_attn.k_norm.weight', 'layers.24.self_attn.k_proj.weight', 'layers.24.self_attn.o_proj.weight', 'layers.24.self_attn.q_norm.weight', 'layers.24.self_attn.q_proj.weight', 'layers.24.self_attn.v_proj.weight', 'layers.25.input_layernorm.weight', 'layers.25.mlp.down_proj.weight', 'layers.25.mlp.gate_proj.weight', 'layers.25.mlp.up_proj.weight', 'layers.25.post_attention_layernorm.weight', 'layers.25.post_feedforward_layernorm.weight', 'layers.25.pre_feedforward_layernorm.weight', 'layers.25.self_attn.k_norm.weight', 'layers.25.self_attn.k_proj.weight', 'layers.25.self_attn.o_proj.weight', 'layers.25.self_attn.q_norm.weight', 'layers.25.self_attn.q_proj.weight', 'layers.25.self_attn.v_proj.weight', 'layers.3.input_layernorm.weight', 'layers.3.mlp.down_proj.weight', 'layers.3.mlp.gate_proj.weight', 'layers.3.mlp.up_proj.weight', 'layers.3.post_attention_layernorm.weight', 'layers.3.post_feedforward_layernorm.weight', 'layers.3.pre_feedforward_layernorm.weight', 'layers.3.self_attn.k_norm.weight', 'layers.3.self_attn.k_proj.weight', 'layers.3.self_attn.o_proj.weight', 'layers.3.self_attn.q_norm.weight', 'layers.3.self_attn.q_proj.weight', 'layers.3.self_attn.v_proj.weight', 'layers.4.input_layernorm.weight', 'layers.4.mlp.down_proj.weight', 'layers.4.mlp.gate_proj.weight', 'layers.4.mlp.up_proj.weight', 'layers.4.post_attention_layernorm.weight', 'layers.4.post_feedforward_layernorm.weight', 'layers.4.pre_feedforward_layernorm.weight', 'layers.4.self_attn.k_norm.weight', 'layers.4.self_attn.k_proj.weight', 'layers.4.self_attn.o_proj.weight', 'layers.4.self_attn.q_norm.weight', 'layers.4.self_attn.q_proj.weight', 'layers.4.self_attn.v_proj.weight', 'layers.5.input_layernorm.weight', 'layers.5.mlp.down_proj.weight', 'layers.5.mlp.gate_proj.weight', 'layers.5.mlp.up_proj.weight', 'layers.5.post_attention_layernorm.weight', 'layers.5.post_feedforward_layernorm.weight', 'layers.5.pre_feedforward_layernorm.weight', 'layers.5.self_attn.k_norm.weight', 'layers.5.self_attn.k_proj.weight', 'layers.5.self_attn.o_proj.weight', 'layers.5.self_attn.q_norm.weight', 'layers.5.self_attn.q_proj.weight', 'layers.5.self_attn.v_proj.weight', 'layers.6.input_layernorm.weight', 'layers.6.mlp.down_proj.weight', 'layers.6.mlp.gate_proj.weight', 'layers.6.mlp.up_proj.weight', 'layers.6.post_attention_layernorm.weight', 'layers.6.post_feedforward_layernorm.weight', 'layers.6.pre_feedforward_layernorm.weight', 'layers.6.self_attn.k_norm.weight', 'layers.6.self_attn.k_proj.weight', 'layers.6.self_attn.o_proj.weight', 'layers.6.self_attn.q_norm.weight', 'layers.6.self_attn.q_proj.weight', 'layers.6.self_attn.v_proj.weight', 'layers.7.input_layernorm.weight', 'layers.7.mlp.down_proj.weight', 'layers.7.mlp.gate_proj.weight', 'layers.7.mlp.up_proj.weight', 'layers.7.post_attention_layernorm.weight', 'layers.7.post_feedforward_layernorm.weight', 'layers.7.pre_feedforward_layernorm.weight', 'layers.7.self_attn.k_norm.weight', 'layers.7.self_attn.k_proj.weight', 'layers.7.self_attn.o_proj.weight', 'layers.7.self_attn.q_norm.weight', 'layers.7.self_attn.q_proj.weight', 'layers.7.self_attn.v_proj.weight', 'layers.8.input_layernorm.weight', 'layers.8.mlp.down_proj.weight', 'layers.8.mlp.gate_proj.weight', 'layers.8.mlp.up_proj.weight', 'layers.8.post_attention_layernorm.weight', 'layers.8.post_feedforward_layernorm.weight', 'layers.8.pre_feedforward_layernorm.weight', 'layers.8.self_attn.k_norm.weight', 'layers.8.self_attn.k_proj.weight', 'layers.8.self_attn.o_proj.weight', 'layers.8.self_attn.q_norm.weight', 'layers.8.self_attn.q_proj.weight', 'layers.8.self_attn.v_proj.weight', 'layers.9.input_layernorm.weight', 'layers.9.mlp.down_proj.weight', 'layers.9.mlp.gate_proj.weight', 'layers.9.mlp.up_proj.weight', 'layers.9.post_attention_layernorm.weight', 'layers.9.post_feedforward_layernorm.weight', 'layers.9.pre_feedforward_layernorm.weight', 'layers.9.self_attn.k_norm.weight', 'layers.9.self_attn.k_proj.weight', 'layers.9.self_attn.o_proj.weight', 'layers.9.self_attn.q_norm.weight', 'layers.9.self_attn.q_proj.weight', 'layers.9.self_attn.v_proj.weight', 'norm.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_final_assessment(results, mlp_analysis):\n",
        "    \"\"\"\n",
        "    Generate final compatibility assessment\n",
        "    \"\"\"\n",
        "    if not results:\n",
        "        print(\"‚ùå INCOMPATIBLE: Could not load model configuration\")\n",
        "        return False\n",
        "\n",
        "    if not mlp_analysis or not mlp_analysis.get(\"found_mlp\", False):\n",
        "        print(\"‚ùå INCOMPATIBLE: Could not analyze MLP structure\")\n",
        "        results[\"issues\"].append(\"‚ùå Could not access MLP layers\")\n",
        "        results[\"compatible\"] = False\n",
        "        return results\n",
        "\n",
        "    # Update results with MLP analysis\n",
        "    results[\"details\"].update(mlp_analysis)\n",
        "\n",
        "    # Determine compatibility\n",
        "    model_type = results[\"details\"][\"model_type\"].lower()\n",
        "    has_glu = mlp_analysis[\"has_glu\"]\n",
        "    expansion_ratio = mlp_analysis[\"expansion_ratio\"]\n",
        "\n",
        "    # Known compatible architectures\n",
        "    compatible_types = [\"llama\", \"mistral\", \"gemma\", \"qwen\", \"phi\"]\n",
        "\n",
        "    if any(comp_type in model_type for comp_type in compatible_types):\n",
        "        results[\"architecture\"] = \"Supported\"\n",
        "        if has_glu and expansion_ratio > 100:\n",
        "            results[\"compatible\"] = True\n",
        "            results[\"recommendations\"].append(f\"‚úÖ Perfect! GLU structure detected with {expansion_ratio:.0f}% expansion\")\n",
        "        else:\n",
        "            results[\"issues\"].append(\"‚ùå No GLU structure found or insufficient expansion\")\n",
        "            if expansion_ratio > 0:\n",
        "                results[\"issues\"].append(f\"‚ö†Ô∏è  Expansion ratio: {expansion_ratio:.0f}% (minimum 100% recommended)\")\n",
        "    else:\n",
        "        results[\"architecture\"] = \"Unknown/Unsupported\"\n",
        "        results[\"issues\"].append(f\"‚ùå Architecture '{model_type}' not yet supported\")\n",
        "        results[\"recommendations\"].append(\"üìß Request support via GitHub issues\")\n",
        "\n",
        "    return results\n",
        "\n",
        "# Generate final assessment\n",
        "final_results = generate_final_assessment(compatibility_results, mlp_analysis)"
      ],
      "metadata": {
        "id": "YtG-A_Q7WvHL"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Final Results"
      ],
      "metadata": {
        "id": "qFQ19R0IW02z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def display_results(results):\n",
        "    \"\"\"\n",
        "    Display the final compatibility results in a clean format\n",
        "    \"\"\"\n",
        "    print(\"=\" * 60)\n",
        "    print(\"üéØ OPTIPFAIR PRUNING COMPATIBILITY REPORT\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # Header\n",
        "    status_emoji = \"‚úÖ\" if results[\"compatible\"] else \"‚ùå\"\n",
        "    status_text = \"COMPATIBLE\" if results[\"compatible\"] else \"NOT COMPATIBLE\"\n",
        "\n",
        "    print(f\"\\n{status_emoji} STATUS: {status_text}\")\n",
        "    print(f\"üèóÔ∏è  ARCHITECTURE: {results['architecture']}\")\n",
        "    print(f\"üì¶ MODEL: {results['model_name']}\")\n",
        "\n",
        "    # Details\n",
        "    print(f\"\\nüìä TECHNICAL DETAILS:\")\n",
        "    details = results[\"details\"]\n",
        "    print(f\"   ‚Ä¢ Model Type: {details.get('model_type', 'unknown')}\")\n",
        "    print(f\"   ‚Ä¢ Layers: {details.get('num_layers', 'unknown')}\")\n",
        "    print(f\"   ‚Ä¢ Hidden Size: {details.get('hidden_size', 'unknown')}\")\n",
        "\n",
        "    if details.get(\"expansion_ratio\", 0) > 0:\n",
        "        print(f\"   ‚Ä¢ MLP Expansion: {details['expansion_ratio']:.0f}%\")\n",
        "        print(f\"   ‚Ä¢ GLU Structure: {'‚úÖ Yes' if details.get('has_glu') else '‚ùå No'}\")\n",
        "\n",
        "    # Issues\n",
        "    if results[\"issues\"]:\n",
        "        print(f\"\\n‚ö†Ô∏è  ISSUES FOUND:\")\n",
        "        for issue in results[\"issues\"]:\n",
        "            print(f\"   {issue}\")\n",
        "\n",
        "    # Recommendations\n",
        "    if results[\"recommendations\"]:\n",
        "        print(f\"\\nüí° RECOMMENDATIONS:\")\n",
        "        for rec in results[\"recommendations\"]:\n",
        "            print(f\"   {rec}\")\n",
        "\n",
        "    # Next steps\n",
        "    print(f\"\\nüöÄ NEXT STEPS:\")\n",
        "    if results[\"compatible\"]:\n",
        "        print(\"   üì¶ Install OptipFair: pip install optipfair\")\n",
        "        print(\"   üìù Check the examples/ folder in OptipFair repository\")\n",
        "        print(\"   üîó https://github.com/peremartra/optipfair\")\n",
        "    else:\n",
        "        print(\"   üìß Open an issue: https://github.com/peremartra/optipfair/issues\")\n",
        "        print(\"   üìö Check supported models: https://github.com/peremartra/optipfair#supported-models\")\n",
        "\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "# Display final results\n",
        "if final_results:\n",
        "    display_results(final_results)\n",
        "else:\n",
        "    print(\"‚ùå Could not complete compatibility check\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2INVBFCZW8v-",
        "outputId": "a10e8bab-6dd2-40ff-9720-8c002aba9e16"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "üéØ OPTIPFAIR PRUNING COMPATIBILITY REPORT\n",
            "============================================================\n",
            "\n",
            "‚úÖ STATUS: COMPATIBLE\n",
            "üèóÔ∏è  ARCHITECTURE: Supported\n",
            "üì¶ MODEL: google/gemma-3-1b-pt\n",
            "\n",
            "üìä TECHNICAL DETAILS:\n",
            "   ‚Ä¢ Model Type: gemma3_text\n",
            "   ‚Ä¢ Layers: 26\n",
            "   ‚Ä¢ Hidden Size: 1152\n",
            "   ‚Ä¢ MLP Expansion: 600%\n",
            "   ‚Ä¢ GLU Structure: ‚úÖ Yes\n",
            "\n",
            "üí° RECOMMENDATIONS:\n",
            "   ‚úÖ Perfect! GLU structure detected with 600% expansion\n",
            "\n",
            "üöÄ NEXT STEPS:\n",
            "   üì¶ Install OptipFair: pip install optipfair\n",
            "   üìù Check the examples/ folder in OptipFair repository\n",
            "   üîó https://github.com/peremartra/optipfair\n",
            "============================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üîó What's Next?\n",
        "\n",
        "### ‚úÖ If your model is compatible:\n",
        "- **Try pruning:** Run the `basic_pruning_mlp.ipynb` notebook  \n",
        "- **Optimize further:** Experiment with different pruning percentages\n",
        "- **Visualize:** Check out `visualization_compatibility_check.ipynb`\n",
        "\n",
        "### ‚ùå If your model is not compatible:\n",
        "- **Request support:** Open an issue on [GitHub](https://github.com/peremartra/optipfair/issues)\n",
        "- **Check updates:** New architectures are added regularly\n",
        "- **Contribute:** Help us add support for your model!\n",
        "\n",
        "---\n",
        "\n",
        "## üìö Learn More\n",
        "\n",
        "- **üìñ Documentation:** [OptipFair GitHub](https://github.com/peremartra/optipfair)  \n",
        "- **üìù Tutorials:** [Large Language Models Course](https://github.com/peremartra/Large-Language-Model-Notebooks-Course)\n",
        "- **üéØ Research:** [GLU Expansion Ratios Paper](https://osf.io/preprints/osf/qgxea)\n",
        "\n",
        "---\n",
        "\n",
        "If you found this notebook useful, the best way to support the OptiPFair project is by **starring it on GitHub**. Your support is a huge help in boosting the project's visibility and reaching more developers and researchers.\n",
        "\n",
        "### ‚û°Ô∏è [**Star OptiPFair on GitHub**](https://github.com/peremartra/optipfair)\n",
        "\n",
        "---\n",
        "You can also follow my work and new projects on:\n",
        "\n",
        "* **[LinkedIn](https://www.linkedin.com/in/pere-martra/)**\n",
        "* **[X / Twitter](https://twitter.com/PereMartra)**"
      ],
      "metadata": {
        "id": "XIvVZY5HXCtk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "yWVm9C7lTbtg"
      },
      "outputs": [],
      "source": []
    }
  ]
}