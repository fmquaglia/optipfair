{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/peremartra/optipfair/blob/main/examples/depth_pruning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "header"
      },
      "source": [
        "#OptiPFair Notebook Series – Example: Depth Pruning\n",
        "\n",
        "![optiPfair Logo](https://github.com/peremartra/optipfair/blob/main/images/optiPfair.png?raw=true)\n",
        "\n",
        "\n",
        "This notebook demonstrates how to use [OptiPFair](https://github.com/peremartra/optipfair) for depth pruning of transformer models by removing entire layers.  \n",
        "This is a more aggressive pruning strategy that can lead to significant efficiency gains.\n",
        "\n",
        "##Recommended Environment\n",
        "\n",
        "- **Platform**: [Google Colab](https://colab.research.google.com)  \n",
        "- **Hardware**: GPU runtime (recommended: T4 or better for 1B–3B models)  \n",
        "- **Dependencies**: Installed automatically in the first cell (optipfair, transformers, torch)\n",
        "\n",
        "##by Pere Martra.\n",
        "\n",
        "- [LinkedIn](https://www.linkedin.com/in/pere-martra)  \n",
        "- [GitHub](https://github.com/peremartra)  \n",
        "- [X / Twitter](https://x.com/peremartra)\n",
        "\n",
        "---\n",
        "\n",
        "> If you find this useful, please ⭐ the [repository](https://github.com/peremartra/optipfair) and share it!\n",
        "---\n",
        "If you want your favorite LLM to create code with optiPfair, you just need to provide it with the file: [**optipfair_llm_reference_manual.txt**](https://github.com/peremartra/optipfair/blob/main/optipfair_llm_reference_manual.txt), which contains all the necessary information for the LLM to become an expert in using the library."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "intro"
      },
      "source": [
        "# Depth Pruning Example\n",
        "\n",
        "This notebook demonstrates how to use OptiPFair for depth pruning of language models.\n",
        "Depth pruning removes entire transformer layers, which is more aggressive than neuron-level pruning\n",
        "but can lead to significant efficiency gains with proper fine-tuning, or recovering Knowledge with Knowledge Distillation from the Base Model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "installation"
      },
      "source": [
        "---\n",
        "## Installation and Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d90ad365-ec3f-4528-ef67-daf81464921c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.0/40.0 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m43.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m40.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m79.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q transformers optipfair torch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "imports"
      },
      "source": [
        "\n",
        "## Import Libraries and Check GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "setup",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b62ca96-8253-4e08-e409-541c58dcc698"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "GPU: Tesla T4\n",
            "GPU Memory: 14.7 GB\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import os\n",
        "import gc\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from optipfair.pruning.depth import prune_model_depth\n",
        "\n",
        "# Check device availability\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "config"
      },
      "source": [
        "## Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "configuration",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9cf89c7f-f6bc-411b-d7b5-4b23120eca6d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Configuration set successfully!\n",
            "Models to test: ['meta-llama/Llama-3.2-1B']\n",
            "Layers to remove: 4\n",
            "Depth pruning percentage: 25%\n",
            "Custom layer indices: [12, 13, 14, 15]\n"
          ]
        }
      ],
      "source": [
        "# List of models to test - you can add more models here\n",
        "# Note: For Colab, stick to smaller models due to memory constraints\n",
        "MODELS_TO_TEST = [\n",
        "    \"meta-llama/Llama-3.2-1B\",\n",
        "    # \"google/gemma-2-2b\",  # Uncomment if you have enough GPU memory\n",
        "    # Add more models here as needed\n",
        "]\n",
        "\n",
        "# Depth pruning configuration - modify these values as needed\n",
        "NUM_LAYERS_TO_REMOVE = 4  # Number of layers to remove\n",
        "DEPTH_PRUNING_PERCENTAGE = 25  # Alternative: percentage of layers to remove (0-100)\n",
        "CUSTOM_LAYER_INDICES = [12, 13, 14, 15]  # Alternative: specific layers to remove\n",
        "\n",
        "# Test prompts for evaluation\n",
        "TEST_PROMPTS = [\n",
        "    \"Paris is the capital of\",\n",
        "    \"The theory of relativity states that\",\n",
        "    \"Machine learning is a field of\",\n",
        "]\n",
        "\n",
        "print(\"Configuration set successfully!\")\n",
        "print(f\"Models to test: {MODELS_TO_TEST}\")\n",
        "print(f\"Layers to remove: {NUM_LAYERS_TO_REMOVE}\")\n",
        "print(f\"Depth pruning percentage: {DEPTH_PRUNING_PERCENTAGE}%\")\n",
        "print(f\"Custom layer indices: {CUSTOM_LAYER_INDICES}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pruning-intro"
      },
      "source": [
        "## Introduction to Depth Pruning\n",
        "\n",
        "---\n",
        "This example demonstrates depth pruning of transformer models.\n",
        "\n",
        "Depth pruning removes entire transformer layers, which is more aggressive than neuron-level pruning but can lead to significant efficiency gains. This approach maintains the model architecture while reducing the total number of layers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "utils"
      },
      "source": [
        "## Utility Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "utility-functions",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d5d85d0-22c6-4ee1-9bec-2e1677e5e1e8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Utility functions defined successfully!\n"
          ]
        }
      ],
      "source": [
        "def count_parameters(model):\n",
        "    \"\"\"Count total parameters in model\"\"\"\n",
        "    return sum(p.numel() for p in model.parameters())\n",
        "\n",
        "def count_layers(model):\n",
        "    \"\"\"Count the number of transformer layers in the model\"\"\"\n",
        "    from optipfair.pruning.utils import get_model_layers\n",
        "    layers = get_model_layers(model)\n",
        "    return len(layers) if layers else 0\n",
        "\n",
        "def test_model_generation(model, tokenizer, prompt, max_length=50):\n",
        "    \"\"\"Test text generation with the model\"\"\"\n",
        "    inputs = tokenizer(prompt, return_tensors='pt').to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            inputs['input_ids'],\n",
        "            attention_mask=inputs['attention_mask'],\n",
        "            max_length=max_length,\n",
        "            num_return_sequences=1,\n",
        "            pad_token_id=tokenizer.pad_token_id,\n",
        "            do_sample=False,\n",
        "            num_beams=3,\n",
        "            early_stopping=True,\n",
        "            no_repeat_ngram_size=2\n",
        "        )\n",
        "\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "def print_model_info(model, model_name, stage=\"\"):\n",
        "    \"\"\"Print basic model information\"\"\"\n",
        "    param_count = count_parameters(model)\n",
        "    layer_count = count_layers(model)\n",
        "    print(f\"{stage} Model: {model_name}\")\n",
        "    print(f\"Parameters: {param_count:,}\")\n",
        "    print(f\"Layers: {layer_count}\")\n",
        "    return param_count, layer_count\n",
        "\n",
        "def cleanup_memory():\n",
        "    \"\"\"Clean up GPU memory - important for Colab\"\"\"\n",
        "    gc.collect()\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "print(\"Utility functions defined successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "params-explanation"
      },
      "source": [
        "\n",
        "## Depth Pruning Parameters Explanation\n",
        "• **model**: The model to be pruned\n",
        "\n",
        "• **num_layers_to_remove**: Number of layers to remove (mutually exclusive with other options)\n",
        "\n",
        "• **layer_indices**: Specific layer indices to remove (mutually exclusive with other options)\n",
        "\n",
        "• **depth_pruning_percentage**: Percentage of layers to remove (0-100) (mutually exclusive with other options)\n",
        "\n",
        "• **layer_selection_method**: Method for selecting layers:\n",
        "  - 'last': Remove the last N layers (recommended for maintaining performance)\n",
        "  - 'custom': Remove specific layers (requires layer_indices)\n",
        "  \n",
        "• **show_progress**: Display progress bar during pruning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "example1"
      },
      "source": [
        "## Example 1 - Depth Pruning by Number of Layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "example-1-function",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99ec8653-8d9d-441b-8f0e-d10c4aaa3be5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Example 1 function defined!\n"
          ]
        }
      ],
      "source": [
        "def example_depth_pruning_by_count(model, tokenizer, model_name):\n",
        "    \"\"\"Example of depth pruning by removing a specific number of layers\"\"\"\n",
        "    print(f\"=== Example 1: Removing {NUM_LAYERS_TO_REMOVE} layers from {model_name} ===\")\n",
        "\n",
        "    # Get original model info\n",
        "    original_params, original_layers = print_model_info(model, model_name, \"Original\")\n",
        "\n",
        "    # Test original model\n",
        "    print(\"\\n--- Original Model Generation ---\")\n",
        "    for prompt in TEST_PROMPTS[:2]:  # Test first 2 prompts\n",
        "        generated = test_model_generation(model, tokenizer, prompt)\n",
        "        print(f\"Prompt: '{prompt}'\")\n",
        "        print(f\"Generated: {generated}\")\n",
        "        print()\n",
        "\n",
        "    # Apply depth pruning by number of layers\n",
        "    pruned_model = prune_model_depth(\n",
        "        model=model,\n",
        "        num_layers_to_remove=NUM_LAYERS_TO_REMOVE,\n",
        "        layer_selection_method=\"last\",  # Remove the last layers\n",
        "        show_progress=True\n",
        "    )\n",
        "\n",
        "    # Get pruned model info\n",
        "    pruned_params, pruned_layers = print_model_info(pruned_model, model_name, \"\\n--- Pruned\")\n",
        "\n",
        "    # Calculate reduction\n",
        "    param_reduction = original_params - pruned_params\n",
        "    param_reduction_pct = (param_reduction / original_params) * 100\n",
        "    layer_reduction = original_layers - pruned_layers\n",
        "\n",
        "    print(f\"\\n--- Pruning Results ---\")\n",
        "    print(f\"Parameter reduction: {param_reduction:,} ({param_reduction_pct:.2f}%)\")\n",
        "    print(f\"Layer reduction: {layer_reduction} layers ({layer_reduction / original_layers * 100:.2f}%)\")\n",
        "\n",
        "    # Test pruned model\n",
        "    print(\"\\n--- Pruned Model Generation ---\")\n",
        "    for prompt in TEST_PROMPTS[:2]:\n",
        "        generated = test_model_generation(pruned_model, tokenizer, prompt)\n",
        "        print(f\"Prompt: '{prompt}'\")\n",
        "        print(f\"Generated: {generated}\")\n",
        "        print()\n",
        "\n",
        "    return pruned_model, {\n",
        "        'original_parameters': original_params,\n",
        "        'pruned_parameters': pruned_params,\n",
        "        'parameter_reduction': param_reduction,\n",
        "        'parameter_reduction_pct': param_reduction_pct,\n",
        "        'original_layers': original_layers,\n",
        "        'pruned_layers': pruned_layers,\n",
        "        'layer_reduction': layer_reduction\n",
        "    }\n",
        "\n",
        "print(\"Example 1 function defined!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "example2"
      },
      "source": [
        "\n",
        "## Example 2 - Depth Pruning by Percentage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "example-2-function",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "954b925a-b786-4f85-ea69-2e589a58ead3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Example 2 function defined!\n"
          ]
        }
      ],
      "source": [
        "def example_depth_pruning_by_percentage(model, tokenizer, model_name):\n",
        "    \"\"\"Example of depth pruning by removing a percentage of layers\"\"\"\n",
        "    print(f\"=== Example 2: Removing {DEPTH_PRUNING_PERCENTAGE}% of layers from {model_name} ===\")\n",
        "\n",
        "    # Get original model info\n",
        "    original_params, original_layers = print_model_info(model, model_name, \"Original\")\n",
        "\n",
        "    # Apply depth pruning by percentage\n",
        "    pruned_model = prune_model_depth(\n",
        "        model=model,\n",
        "        depth_pruning_percentage=DEPTH_PRUNING_PERCENTAGE,\n",
        "        layer_selection_method=\"last\",  # Remove the last layers\n",
        "        show_progress=True\n",
        "    )\n",
        "\n",
        "    # Get pruned model info\n",
        "    pruned_params, pruned_layers = print_model_info(pruned_model, model_name, \"\\n--- Pruned\")\n",
        "\n",
        "    # Calculate reduction\n",
        "    param_reduction = original_params - pruned_params\n",
        "    param_reduction_pct = (param_reduction / original_params) * 100\n",
        "    layer_reduction = original_layers - pruned_layers\n",
        "\n",
        "    print(f\"\\n--- Pruning Results ---\")\n",
        "    print(f\"Target layer reduction: {DEPTH_PRUNING_PERCENTAGE}%\")\n",
        "    print(f\"Actual layer reduction: {layer_reduction} layers ({layer_reduction / original_layers * 100:.2f}%)\")\n",
        "    print(f\"Parameter reduction: {param_reduction:,} ({param_reduction_pct:.2f}%)\")\n",
        "\n",
        "    # Test pruned model with one prompt\n",
        "    print(\"\\n--- Pruned Model Generation ---\")\n",
        "    prompt = TEST_PROMPTS[0]\n",
        "    generated = test_model_generation(pruned_model, tokenizer, prompt)\n",
        "    print(f\"Prompt: '{prompt}'\")\n",
        "    print(f\"Generated: {generated}\")\n",
        "    print()\n",
        "\n",
        "    return pruned_model, {\n",
        "        'original_parameters': original_params,\n",
        "        'pruned_parameters': pruned_params,\n",
        "        'parameter_reduction': param_reduction,\n",
        "        'parameter_reduction_pct': param_reduction_pct,\n",
        "        'original_layers': original_layers,\n",
        "        'pruned_layers': pruned_layers,\n",
        "        'layer_reduction': layer_reduction\n",
        "    }\n",
        "\n",
        "print(\"Example 2 function defined!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "example3"
      },
      "source": [
        "\n",
        "## Example 3 - Custom Layer Selection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "example-3-function",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec4485e5-6618-4169-9ffc-83e58fafccf6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Example 3 function defined!\n"
          ]
        }
      ],
      "source": [
        "def example_depth_pruning_custom_layers(model, tokenizer, model_name):\n",
        "    \"\"\"Example of depth pruning by removing specific layers\"\"\"\n",
        "    print(f\"=== Example 3: Removing custom layers {CUSTOM_LAYER_INDICES} from {model_name} ===\")\n",
        "\n",
        "    # Get original model info\n",
        "    original_params, original_layers = print_model_info(model, model_name, \"Original\")\n",
        "\n",
        "    # Validate custom indices\n",
        "    valid_indices = [i for i in CUSTOM_LAYER_INDICES if 0 <= i < original_layers]\n",
        "    if len(valid_indices) != len(CUSTOM_LAYER_INDICES):\n",
        "        print(f\"Warning: Some custom indices are invalid. Using valid indices: {valid_indices}\")\n",
        "\n",
        "    if not valid_indices:\n",
        "        print(\"Error: No valid layer indices to remove\")\n",
        "        return None, None\n",
        "\n",
        "    # Apply depth pruning with custom layer indices\n",
        "    pruned_model = prune_model_depth(\n",
        "        model=model,\n",
        "        layer_indices=valid_indices,\n",
        "        show_progress=True\n",
        "    )\n",
        "\n",
        "    # Get pruned model info\n",
        "    pruned_params, pruned_layers = print_model_info(pruned_model, model_name, \"\\n--- Pruned\")\n",
        "\n",
        "    # Calculate reduction\n",
        "    param_reduction = original_params - pruned_params\n",
        "    param_reduction_pct = (param_reduction / original_params) * 100\n",
        "    layer_reduction = len(valid_indices)\n",
        "\n",
        "    print(f\"\\n--- Pruning Results ---\")\n",
        "    print(f\"Removed layers: {valid_indices}\")\n",
        "    print(f\"Layer reduction: {layer_reduction} layers ({layer_reduction / original_layers * 100:.2f}%)\")\n",
        "    print(f\"Parameter reduction: {param_reduction:,} ({param_reduction_pct:.2f}%)\")\n",
        "\n",
        "    # Test pruned model with one prompt\n",
        "    print(\"\\n--- Pruned Model Generation ---\")\n",
        "    prompt = TEST_PROMPTS[0]\n",
        "    generated = test_model_generation(pruned_model, tokenizer, prompt)\n",
        "    print(f\"Prompt: '{prompt}'\")\n",
        "    print(f\"Generated: {generated}\")\n",
        "    print()\n",
        "\n",
        "    return pruned_model, {\n",
        "        'original_parameters': original_params,\n",
        "        'pruned_parameters': pruned_params,\n",
        "        'parameter_reduction': param_reduction,\n",
        "        'parameter_reduction_pct': param_reduction_pct,\n",
        "        'original_layers': original_layers,\n",
        "        'pruned_layers': pruned_layers,\n",
        "        'layer_reduction': layer_reduction,\n",
        "        'removed_indices': valid_indices\n",
        "    }\n",
        "\n",
        "print(\"Example 3 function defined!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "run-example1"
      },
      "source": [
        "## Run Example 1 - Depth Pruning by Layer Count"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Starting Example 1: Depth Pruning by Layer Count\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Process the first model in the list\n",
        "model_name = MODELS_TO_TEST[0]\n",
        "print(f\"Loading model: {model_name}\")\n",
        "\n",
        "# Load model and tokenizer\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    torch_dtype=torch.float16 if device.type == 'cuda' else torch.float32,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "nj7Gk-ufEAu7",
        "outputId": "12f38445-a95e-45d4-9fd4-e984afedcca2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Example 1: Depth Pruning by Layer Count\n",
            "==================================================\n",
            "Loading model: meta-llama/Llama-3.2-1B\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "run-example-1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "baaf8316-e65e-4772-b1c3-cbc6fb5e6c9b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Example 1: Removing 4 layers from meta-llama/Llama-3.2-1B ===\n",
            "Original Model: meta-llama/Llama-3.2-1B\n",
            "Parameters: 1,235,814,400\n",
            "Layers: 16\n",
            "\n",
            "--- Original Model Generation ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt: 'Paris is the capital of'\n",
            "Generated: Paris is the capital of France and the largest city in the country. It is located on the River Seine and is one of the most popular tourist destinations in Europe. The city has a population of over 2.2 million people, making\n",
            "\n",
            "Prompt: 'The theory of relativity states that'\n",
            "Generated: The theory of relativity states that the speed of light is the same in all inertial frames of reference. In other words, light always travels at a constant speed, regardless of the motion of either the source or the observer. This means that\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Removing layers: 100%|██████████| 16/16 [00:00<00:00, 77492.91it/s]\n",
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Pruned Model: meta-llama/Llama-3.2-1B\n",
            "Parameters: 992,528,384\n",
            "Layers: 12\n",
            "\n",
            "--- Pruning Results ---\n",
            "Parameter reduction: 243,286,016 (19.69%)\n",
            "Layer reduction: 4 layers (25.00%)\n",
            "\n",
            "--- Pruned Model Generation ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt: 'Paris is the capital of'\n",
            "Generated: Paris is the capital of France situated situated north-westernmost corner thereof bordered by France westward-facing Alps mountain mountainountainountain mountain-mount mountmount mount mount-mount mountain Mountains mountains mountains mountainMountain mountain Mountain Mountains Mountains mountain山山 mountain mountains Mountain mountains\n",
            "\n",
            "Prompt: 'The theory of relativity states that'\n",
            "Generated: The theory of relativity states that speeds speedspeed-speed-speedspeedspeeds fours fours-fourteenteenteentheentheenth-century philosopher philosopher philosophers philosopherosopher philosopherosophosophopo philosophosopherosopher physicist physicist physicists physicist scientist scientistscientScientScient Scientist scientist Scientist\n",
            "\n",
            "\n",
            "Example 1 completed! Parameter reduction: 19.69%\n"
          ]
        }
      ],
      "source": [
        "# Set pad token if not present\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Run Example 1\n",
        "pruned_model_1, stats_1 = example_depth_pruning_by_count(model, tokenizer, model_name)\n",
        "\n",
        "# Store stats for summary\n",
        "results = [{\n",
        "    'model': model_name,\n",
        "    'method': 'Layer Count',\n",
        "    'param_reduction': stats_1['parameter_reduction_pct'],\n",
        "    'layer_reduction': stats_1['layer_reduction'],\n",
        "    'layers_removed': f\"{stats_1['layer_reduction']}/{stats_1['original_layers']}\"\n",
        "}]\n",
        "\n",
        "print(f\"\\nExample 1 completed! Parameter reduction: {stats_1['parameter_reduction_pct']:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "run-example2"
      },
      "source": [
        "## Run Example 2 - Depth Pruning by Percentage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "run-example-2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b8bd934-4fa5-40d1-c216-fa0c6cd51a75"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Example 2: Depth Pruning by Percentage\n",
            "==================================================\n",
            "=== Example 2: Removing 25% of layers from meta-llama/Llama-3.2-1B ===\n",
            "Original Model: meta-llama/Llama-3.2-1B\n",
            "Parameters: 1,235,814,400\n",
            "Layers: 16\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Removing layers: 100%|██████████| 16/16 [00:00<00:00, 81049.35it/s]\n",
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Pruned Model: meta-llama/Llama-3.2-1B\n",
            "Parameters: 992,528,384\n",
            "Layers: 12\n",
            "\n",
            "--- Pruning Results ---\n",
            "Target layer reduction: 25%\n",
            "Actual layer reduction: 4 layers (25.00%)\n",
            "Parameter reduction: 243,286,016 (19.69%)\n",
            "\n",
            "--- Pruned Model Generation ---\n",
            "Prompt: 'Paris is the capital of'\n",
            "Generated: Paris is the capital of France situated situated north-westernmost corner thereof bordered by France westward-facing Alps mountain mountainountainountain mountain-mount mountmount mount mount-mount mountain Mountains mountains mountains mountainMountain mountain Mountain Mountains Mountains mountain山山 mountain mountains Mountain mountains\n",
            "\n",
            "\n",
            "Example 2 completed! Parameter reduction: 19.69%\n"
          ]
        }
      ],
      "source": [
        "print(\"Starting Example 2: Depth Pruning by Percentage\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Clean up memory from previous example\n",
        "cleanup_memory()\n",
        "\n",
        "# Reload model for second example (since first one was modified)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    torch_dtype=torch.float16 if device.type == 'cuda' else torch.float32,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "# Run Example 2\n",
        "pruned_model_2, stats_2 = example_depth_pruning_by_percentage(model, tokenizer, model_name)\n",
        "\n",
        "# Add to results\n",
        "results.append({\n",
        "    'model': model_name,\n",
        "    'method': 'Percentage',\n",
        "    'param_reduction': stats_2['parameter_reduction_pct'],\n",
        "    'layer_reduction': stats_2['layer_reduction'],\n",
        "    'layers_removed': f\"{stats_2['layer_reduction']}/{stats_2['original_layers']}\"\n",
        "})\n",
        "\n",
        "print(f\"\\nExample 2 completed! Parameter reduction: {stats_2['parameter_reduction_pct']:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "run-example3"
      },
      "source": [
        "## Run Example 3 - Custom Layer Selection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "run-example-3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6a8fcb4-1bee-4120-c8c8-deb8c5498dfd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Example 3: Custom Layer Selection\n",
            "==================================================\n",
            "=== Example 3: Removing custom layers [12, 13, 14, 15] from meta-llama/Llama-3.2-1B ===\n",
            "Original Model: meta-llama/Llama-3.2-1B\n",
            "Parameters: 1,235,814,400\n",
            "Layers: 16\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Removing layers: 100%|██████████| 16/16 [00:00<00:00, 64965.02it/s]\n",
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Pruned Model: meta-llama/Llama-3.2-1B\n",
            "Parameters: 992,528,384\n",
            "Layers: 12\n",
            "\n",
            "--- Pruning Results ---\n",
            "Removed layers: [12, 13, 14, 15]\n",
            "Layer reduction: 4 layers (25.00%)\n",
            "Parameter reduction: 243,286,016 (19.69%)\n",
            "\n",
            "--- Pruned Model Generation ---\n",
            "Prompt: 'Paris is the capital of'\n",
            "Generated: Paris is the capital of France situated situated north-westernmost corner thereof bordered by France westward-facing Alps mountain mountainountainountain mountain-mount mountmount mount mount-mount mountain Mountains mountains mountains mountainMountain mountain Mountain Mountains Mountains mountain山山 mountain mountains Mountain mountains\n",
            "\n",
            "\n",
            "Example 3 completed! Parameter reduction: 19.69%\n"
          ]
        }
      ],
      "source": [
        "print(\"Starting Example 3: Custom Layer Selection\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Clean up memory from previous example\n",
        "cleanup_memory()\n",
        "\n",
        "# Reload model for third example (since previous ones were modified)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    torch_dtype=torch.float16 if device.type == 'cuda' else torch.float32,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "# Run Example 3\n",
        "pruned_model_3, stats_3 = example_depth_pruning_custom_layers(model, tokenizer, model_name)\n",
        "\n",
        "if stats_3 is not None:\n",
        "    # Add to results\n",
        "    results.append({\n",
        "        'model': model_name,\n",
        "        'method': 'Custom Layers',\n",
        "        'param_reduction': stats_3['parameter_reduction_pct'],\n",
        "        'layer_reduction': stats_3['layer_reduction'],\n",
        "        'layers_removed': f\"{stats_3['layer_reduction']}/{stats_3['original_layers']}\",\n",
        "        'custom_indices': stats_3['removed_indices']\n",
        "    })\n",
        "\n",
        "    print(f\"\\nExample 3 completed! Parameter reduction: {stats_3['parameter_reduction_pct']:.2f}%\")\n",
        "else:\n",
        "    print(\"\\nExample 3 skipped due to invalid layer indices.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "summary"
      },
      "source": [
        "## Results Summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "results-summary",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05dbb494-a40f-42fc-ce55-1eb0d5c66440"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "DEPTH PRUNING RESULTS SUMMARY\n",
            "======================================================================\n",
            "Model                          Method          Param Reduction Layers Removed \n",
            "----------------------------------------------------------------------\n",
            "meta-llama/Llama-3.2-1B        Layer Count     19.69          % 4/16           \n",
            "meta-llama/Llama-3.2-1B        Percentage      19.69          % 4/16           \n",
            "meta-llama/Llama-3.2-1B        Custom Layers   19.69          % 4/16           \n",
            "                                               Custom indices: [12, 13, 14, 15]\n",
            "\n",
            "Total examples tested: 3\n",
            "Depth pruning examples completed successfully!\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"DEPTH PRUNING RESULTS SUMMARY\")\n",
        "print(\"=\"*70)\n",
        "print(f\"{'Model':<30} {'Method':<15} {'Param Reduction':<15} {'Layers Removed':<15}\")\n",
        "print(\"-\" * 70)\n",
        "\n",
        "for result in results:\n",
        "    print(f\"{result['model']:<30} {result['method']:<15} {result['param_reduction']:<15.2f}% {result['layers_removed']:<15}\")\n",
        "    if 'custom_indices' in result:\n",
        "        print(f\"{'':>30} {'':>15} Custom indices: {result['custom_indices']}\")\n",
        "\n",
        "print(f\"\\nTotal examples tested: {len(results)}\")\n",
        "print(\"Depth pruning examples completed successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "success"
      },
      "source": [
        "---\n",
        "## ✅ Success! What's Next?\n",
        "\n",
        "Congratulations! You've successfully performed depth pruning on a transformer model and seen how OptiPFair makes layer removal simple and configurable.\n",
        "\n",
        "**Key takeaways from depth pruning:**\n",
        "- Depth pruning removes entire layers, leading to significant parameter reduction\n",
        "- The \"last\" layer selection method is generally recommended for maintaining performance\n",
        "- Custom layer selection gives you fine-grained control over which layers to remove\n",
        "- Percentage-based pruning allows for consistent reduction ratios across different models\n",
        "\n",
        "If you found this notebook useful, the best way to support the OptiPFair project is by **starring it on GitHub**. Your support is a huge help in boosting the project's visibility and reaching more developers and researchers.\n",
        "\n",
        "### ➡️ [**Star OptiPFair on GitHub**](https://github.com/peremartra/optipfair)\n",
        "\n",
        "**Note on the results.**\n",
        "\n",
        "If you used the default settings of the example notebook, you might notice that the pruned model's performance is far from optimal. We just removed 25% of the layers from a small model without performing any knowledge recovery, such as fine-tuning with LoRA or Knowledge Distillation from the base model.\n",
        "\n",
        "The `optiPfair` library is not currently equipped to handle knowledge recovery tasks, but you can find an example of Knowledge Distillation in the [pruning & optimization](https://github.com/peremartra/Large-Language-Model-Notebooks-Course/tree/main/6-PRUNING) section of the [LLM course](https://github.com/peremartra/Large-Language-Model-Notebooks-Course) I maintain on Github.\n",
        "* [Knowledge Distillation Notebook](https://github.com/peremartra/Large-Language-Model-Notebooks-Course/blob/main/6-PRUNING/7_1_knowledge_distillation_Llama.ipynb)\n",
        "\n",
        "---\n",
        "You can also follow my work and new projects on:\n",
        "\n",
        "* **[LinkedIn](https://www.linkedin.com/in/pere-martra/)**\n",
        "* **[X / Twitter](https://twitter.com/PereMartra)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "empty"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}