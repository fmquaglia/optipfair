{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "colab-badge",
      "metadata": {
        "id": "colab-badge"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/peremartra/optipfair/blob/main/examples/depth_pruning_example.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "header",
      "metadata": {
        "id": "header"
      },
      "source": [
        "#OptiPFair Notebook Series – Example: Depth Pruning\n",
        "\n",
        "![optiPfair Logo](https://github.com/peremartra/optipfair/blob/main/images/optiPfair.png?raw=true)\n",
        "\n",
        "\n",
        "This notebook demonstrates how to use [OptiPFair](https://github.com/peremartra/optipfair) for depth pruning of transformer models by removing entire layers.  \n",
        "This is a more aggressive pruning strategy that can lead to significant efficiency gains.\n",
        "\n",
        "##Recommended Environment\n",
        "\n",
        "- **Platform**: [Google Colab](https://colab.research.google.com)  \n",
        "- **Hardware**: GPU runtime (recommended: T4 or better for 1B–3B models)  \n",
        "- **Dependencies**: Installed automatically in the first cell (optipfair, transformers, torch)\n",
        "\n",
        "##by Pere Martra.\n",
        "\n",
        "- [LinkedIn](https://www.linkedin.com/in/pere-martra)  \n",
        "- [GitHub](https://github.com/peremartra)  \n",
        "- [X / Twitter](https://x.com/peremartra)\n",
        "\n",
        "---\n",
        "\n",
        "> If you find this useful, please ⭐ the [repository](https://github.com/peremartra/optipfair) and share it!\n",
        "---\n",
        "If you want your favorite LLM to create code with optiPfair, you just need to provide it with the file: [**optipfair_llm_reference_manual.txt**](https://github.com/peremartra/optipfair/blob/main/optipfair_llm_reference_manual.txt), which contains all the necessary information for the LLM to become an expert in using the library."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "intro",
      "metadata": {
        "id": "intro"
      },
      "source": [
        "# Depth Pruning Example\n",
        "\n",
        "This notebook demonstrates how to use OptiPFair for depth pruning of language models.\n",
        "Depth pruning removes entire transformer layers, which is more aggressive than neuron-level pruning\n",
        "but can lead to significant efficiency gains with proper fine-tuning.\n",
        "\n",
        "Author: Pere Martra\n",
        "\n",
        "Designed for Google Colab - GPU runtime recommended"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "installation",
      "metadata": {
        "id": "installation"
      },
      "source": [
        "---\n",
        "## Installation and Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "install",
      "metadata": {
        "id": "install"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers optipfair torch"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "imports",
      "metadata": {
        "id": "imports"
      },
      "source": [
        "\n",
        "## Import Libraries and Check GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "setup",
      "metadata": {
        "id": "setup"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import os\n",
        "import gc\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from optipfair.pruning.depth import prune_model_depth\n",
        "\n",
        "# Check device availability\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "config",
      "metadata": {
        "id": "config"
      },
      "source": [
        "## Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "configuration",
      "metadata": {
        "id": "configuration"
      },
      "outputs": [],
      "source": [
        "# List of models to test - you can add more models here\n",
        "# Note: For Colab, stick to smaller models due to memory constraints\n",
        "MODELS_TO_TEST = [\n",
        "    \"meta-llama/Llama-3.2-1B\",\n",
        "    # \"google/gemma-2-2b\",  # Uncomment if you have enough GPU memory\n",
        "    # Add more models here as needed\n",
        "]\n",
        "\n",
        "# Depth pruning configuration - modify these values as needed\n",
        "NUM_LAYERS_TO_REMOVE = 4  # Number of layers to remove\n",
        "DEPTH_PRUNING_PERCENTAGE = 25  # Alternative: percentage of layers to remove (0-100)\n",
        "CUSTOM_LAYER_INDICES = [12, 13, 14, 15]  # Alternative: specific layers to remove\n",
        "\n",
        "# Test prompts for evaluation\n",
        "TEST_PROMPTS = [\n",
        "    \"Paris is the capital of\",\n",
        "    \"The theory of relativity states that\",\n",
        "    \"Machine learning is a field of\",\n",
        "]\n",
        "\n",
        "print(\"Configuration set successfully!\")\n",
        "print(f\"Models to test: {MODELS_TO_TEST}\")\n",
        "print(f\"Layers to remove: {NUM_LAYERS_TO_REMOVE}\")\n",
        "print(f\"Depth pruning percentage: {DEPTH_PRUNING_PERCENTAGE}%\")\n",
        "print(f\"Custom layer indices: {CUSTOM_LAYER_INDICES}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "pruning-intro",
      "metadata": {
        "id": "pruning-intro"
      },
      "source": [
        "## Introduction to Depth Pruning\n",
        "\n",
        "---\n",
        "This example demonstrates depth pruning of transformer models.\n",
        "\n",
        "Depth pruning removes entire transformer layers, which is more aggressive than neuron-level pruning but can lead to significant efficiency gains. This approach maintains the model architecture while reducing the total number of layers."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "utils",
      "metadata": {
        "id": "utils"
      },
      "source": [
        "## Utility Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "utility-functions",
      "metadata": {
        "id": "utility-functions"
      },
      "outputs": [],
      "source": [
        "def count_parameters(model):\n",
        "    \"\"\"Count total parameters in model\"\"\"\n",
        "    return sum(p.numel() for p in model.parameters())\n",
        "\n",
        "def count_layers(model):\n",
        "    \"\"\"Count the number of transformer layers in the model\"\"\"\n",
        "    from optipfair.pruning.utils import get_model_layers\n",
        "    layers = get_model_layers(model)\n",
        "    return len(layers) if layers else 0\n",
        "\n",
        "def test_model_generation(model, tokenizer, prompt, max_length=50):\n",
        "    \"\"\"Test text generation with the model\"\"\"\n",
        "    inputs = tokenizer(prompt, return_tensors='pt').to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            inputs['input_ids'],\n",
        "            attention_mask=inputs['attention_mask'],\n",
        "            max_length=max_length,\n",
        "            num_return_sequences=1,\n",
        "            pad_token_id=tokenizer.pad_token_id,\n",
        "            do_sample=False,\n",
        "            num_beams=3,\n",
        "            early_stopping=True,\n",
        "            no_repeat_ngram_size=2\n",
        "        )\n",
        "\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "def print_model_info(model, model_name, stage=\"\"):\n",
        "    \"\"\"Print basic model information\"\"\"\n",
        "    param_count = count_parameters(model)\n",
        "    layer_count = count_layers(model)\n",
        "    print(f\"{stage} Model: {model_name}\")\n",
        "    print(f\"Parameters: {param_count:,}\")\n",
        "    print(f\"Layers: {layer_count}\")\n",
        "    return param_count, layer_count\n",
        "\n",
        "def cleanup_memory():\n",
        "    \"\"\"Clean up GPU memory - important for Colab\"\"\"\n",
        "    gc.collect()\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "print(\"Utility functions defined successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "params-explanation",
      "metadata": {
        "id": "params-explanation"
      },
      "source": [
        "\n",
        "## Depth Pruning Parameters Explanation\n",
        "• **model**: The model to be pruned\n",
        "\n",
        "• **num_layers_to_remove**: Number of layers to remove (mutually exclusive with other options)\n",
        "\n",
        "• **layer_indices**: Specific layer indices to remove (mutually exclusive with other options)\n",
        "\n",
        "• **depth_pruning_percentage**: Percentage of layers to remove (0-100) (mutually exclusive with other options)\n",
        "\n",
        "• **layer_selection_method**: Method for selecting layers:\n",
        "  - 'last': Remove the last N layers (recommended for maintaining performance)\n",
        "  - 'custom': Remove specific layers (requires layer_indices)\n",
        "  \n",
        "• **show_progress**: Display progress bar during pruning"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "example1",
      "metadata": {
        "id": "example1"
      },
      "source": [
        "## Example 1 - Depth Pruning by Number of Layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "example-1-function",
      "metadata": {
        "id": "example-1-function"
      },
      "outputs": [],
      "source": [
        "def example_depth_pruning_by_count(model, tokenizer, model_name):\n",
        "    \"\"\"Example of depth pruning by removing a specific number of layers\"\"\"\n",
        "    print(f\"=== Example 1: Removing {NUM_LAYERS_TO_REMOVE} layers from {model_name} ===\")\n",
        "\n",
        "    # Get original model info\n",
        "    original_params, original_layers = print_model_info(model, model_name, \"Original\")\n",
        "\n",
        "    # Test original model\n",
        "    print(\"\\n--- Original Model Generation ---\")\n",
        "    for prompt in TEST_PROMPTS[:2]:  # Test first 2 prompts\n",
        "        generated = test_model_generation(model, tokenizer, prompt)\n",
        "        print(f\"Prompt: '{prompt}'\")\n",
        "        print(f\"Generated: {generated}\")\n",
        "        print()\n",
        "\n",
        "    # Apply depth pruning by number of layers\n",
        "    pruned_model = prune_model_depth(\n",
        "        model=model,\n",
        "        num_layers_to_remove=NUM_LAYERS_TO_REMOVE,\n",
        "        layer_selection_method=\"last\",  # Remove the last layers\n",
        "        show_progress=True\n",
        "    )\n",
        "\n",
        "    # Get pruned model info\n",
        "    pruned_params, pruned_layers = print_model_info(pruned_model, model_name, \"\\n--- Pruned\")\n",
        "\n",
        "    # Calculate reduction\n",
        "    param_reduction = original_params - pruned_params\n",
        "    param_reduction_pct = (param_reduction / original_params) * 100\n",
        "    layer_reduction = original_layers - pruned_layers\n",
        "    \n",
        "    print(f\"\\n--- Pruning Results ---\")\n",
        "    print(f\"Parameter reduction: {param_reduction:,} ({param_reduction_pct:.2f}%)\")\n",
        "    print(f\"Layer reduction: {layer_reduction} layers ({layer_reduction / original_layers * 100:.2f}%)\")\n",
        "\n",
        "    # Test pruned model\n",
        "    print(\"\\n--- Pruned Model Generation ---\")\n",
        "    for prompt in TEST_PROMPTS[:2]:\n",
        "        generated = test_model_generation(pruned_model, tokenizer, prompt)\n",
        "        print(f\"Prompt: '{prompt}'\")\n",
        "        print(f\"Generated: {generated}\")\n",
        "        print()\n",
        "\n",
        "    return pruned_model, {\n",
        "        'original_parameters': original_params,\n",
        "        'pruned_parameters': pruned_params,\n",
        "        'parameter_reduction': param_reduction,\n",
        "        'parameter_reduction_pct': param_reduction_pct,\n",
        "        'original_layers': original_layers,\n",
        "        'pruned_layers': pruned_layers,\n",
        "        'layer_reduction': layer_reduction\n",
        "    }\n",
        "\n",
        "print(\"Example 1 function defined!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "example2",
      "metadata": {
        "id": "example2"
      },
      "source": [
        "\n",
        "## Example 2 - Depth Pruning by Percentage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "example-2-function",
      "metadata": {
        "id": "example-2-function"
      },
      "outputs": [],
      "source": [
        "def example_depth_pruning_by_percentage(model, tokenizer, model_name):\n",
        "    \"\"\"Example of depth pruning by removing a percentage of layers\"\"\"\n",
        "    print(f\"=== Example 2: Removing {DEPTH_PRUNING_PERCENTAGE}% of layers from {model_name} ===\")\n",
        "\n",
        "    # Get original model info\n",
        "    original_params, original_layers = print_model_info(model, model_name, \"Original\")\n",
        "\n",
        "    # Apply depth pruning by percentage\n",
        "    pruned_model = prune_model_depth(\n",
        "        model=model,\n",
        "        depth_pruning_percentage=DEPTH_PRUNING_PERCENTAGE,\n",
        "        layer_selection_method=\"last\",  # Remove the last layers\n",
        "        show_progress=True\n",
        "    )\n",
        "\n",
        "    # Get pruned model info\n",
        "    pruned_params, pruned_layers = print_model_info(pruned_model, model_name, \"\\n--- Pruned\")\n",
        "\n",
        "    # Calculate reduction\n",
        "    param_reduction = original_params - pruned_params\n",
        "    param_reduction_pct = (param_reduction / original_params) * 100\n",
        "    layer_reduction = original_layers - pruned_layers\n",
        "    \n",
        "    print(f\"\\n--- Pruning Results ---\")\n",
        "    print(f\"Target layer reduction: {DEPTH_PRUNING_PERCENTAGE}%\")\n",
        "    print(f\"Actual layer reduction: {layer_reduction} layers ({layer_reduction / original_layers * 100:.2f}%)\")\n",
        "    print(f\"Parameter reduction: {param_reduction:,} ({param_reduction_pct:.2f}%)\")\n",
        "\n",
        "    # Test pruned model with one prompt\n",
        "    print(\"\\n--- Pruned Model Generation ---\")\n",
        "    prompt = TEST_PROMPTS[0]\n",
        "    generated = test_model_generation(pruned_model, tokenizer, prompt)\n",
        "    print(f\"Prompt: '{prompt}'\")\n",
        "    print(f\"Generated: {generated}\")\n",
        "    print()\n",
        "\n",
        "    return pruned_model, {\n",
        "        'original_parameters': original_params,\n",
        "        'pruned_parameters': pruned_params,\n",
        "        'parameter_reduction': param_reduction,\n",
        "        'parameter_reduction_pct': param_reduction_pct,\n",
        "        'original_layers': original_layers,\n",
        "        'pruned_layers': pruned_layers,\n",
        "        'layer_reduction': layer_reduction\n",
        "    }\n",
        "\n",
        "print(\"Example 2 function defined!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "example3",
      "metadata": {
        "id": "example3"
      },
      "source": [
        "\n",
        "## Example 3 - Custom Layer Selection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "example-3-function",
      "metadata": {
        "id": "example-3-function"
      },
      "outputs": [],
      "source": [
        "def example_depth_pruning_custom_layers(model, tokenizer, model_name):\n",
        "    \"\"\"Example of depth pruning by removing specific layers\"\"\"\n",
        "    print(f\"=== Example 3: Removing custom layers {CUSTOM_LAYER_INDICES} from {model_name} ===\")\n",
        "\n",
        "    # Get original model info\n",
        "    original_params, original_layers = print_model_info(model, model_name, \"Original\")\n",
        "    \n",
        "    # Validate custom indices\n",
        "    valid_indices = [i for i in CUSTOM_LAYER_INDICES if 0 <= i < original_layers]\n",
        "    if len(valid_indices) != len(CUSTOM_LAYER_INDICES):\n",
        "        print(f\"Warning: Some custom indices are invalid. Using valid indices: {valid_indices}\")\n",
        "    \n",
        "    if not valid_indices:\n",
        "        print(\"Error: No valid layer indices to remove\")\n",
        "        return None, None\n",
        "\n",
        "    # Apply depth pruning with custom layer indices\n",
        "    pruned_model = prune_model_depth(\n",
        "        model=model,\n",
        "        layer_indices=valid_indices,\n",
        "        show_progress=True\n",
        "    )\n",
        "\n",
        "    # Get pruned model info\n",
        "    pruned_params, pruned_layers = print_model_info(pruned_model, model_name, \"\\n--- Pruned\")\n",
        "\n",
        "    # Calculate reduction\n",
        "    param_reduction = original_params - pruned_params\n",
        "    param_reduction_pct = (param_reduction / original_params) * 100\n",
        "    layer_reduction = len(valid_indices)\n",
        "    \n",
        "    print(f\"\\n--- Pruning Results ---\")\n",
        "    print(f\"Removed layers: {valid_indices}\")\n",
        "    print(f\"Layer reduction: {layer_reduction} layers ({layer_reduction / original_layers * 100:.2f}%)\")\n",
        "    print(f\"Parameter reduction: {param_reduction:,} ({param_reduction_pct:.2f}%)\")\n",
        "\n",
        "    # Test pruned model with one prompt\n",
        "    print(\"\\n--- Pruned Model Generation ---\")\n",
        "    prompt = TEST_PROMPTS[0]\n",
        "    generated = test_model_generation(pruned_model, tokenizer, prompt)\n",
        "    print(f\"Prompt: '{prompt}'\")\n",
        "    print(f\"Generated: {generated}\")\n",
        "    print()\n",
        "\n",
        "    return pruned_model, {\n",
        "        'original_parameters': original_params,\n",
        "        'pruned_parameters': pruned_params,\n",
        "        'parameter_reduction': param_reduction,\n",
        "        'parameter_reduction_pct': param_reduction_pct,\n",
        "        'original_layers': original_layers,\n",
        "        'pruned_layers': pruned_layers,\n",
        "        'layer_reduction': layer_reduction,\n",
        "        'removed_indices': valid_indices\n",
        "    }\n",
        "\n",
        "print(\"Example 3 function defined!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "run-example1",
      "metadata": {
        "id": "run-example1"
      },
      "source": [
        "## Run Example 1 - Depth Pruning by Layer Count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "run-example-1",
      "metadata": {
        "id": "run-example-1"
      },
      "outputs": [],
      "source": [
        "print(\"Starting Example 1: Depth Pruning by Layer Count\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Process the first model in the list\n",
        "model_name = MODELS_TO_TEST[0]\n",
        "print(f\"Loading model: {model_name}\")\n",
        "\n",
        "# Load model and tokenizer\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    torch_dtype=torch.float16 if device.type == 'cuda' else torch.float32,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Set pad token if not present\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Run Example 1\n",
        "pruned_model_1, stats_1 = example_depth_pruning_by_count(model, tokenizer, model_name)\n",
        "\n",
        "# Store stats for summary\n",
        "results = [{\n",
        "    'model': model_name,\n",
        "    'method': 'Layer Count',\n",
        "    'param_reduction': stats_1['parameter_reduction_pct'],\n",
        "    'layer_reduction': stats_1['layer_reduction'],\n",
        "    'layers_removed': f\"{stats_1['layer_reduction']}/{stats_1['original_layers']}\"\n",
        "}]\n",
        "\n",
        "print(f\"\\nExample 1 completed! Parameter reduction: {stats_1['parameter_reduction_pct']:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "run-example2",
      "metadata": {
        "id": "run-example2"
      },
      "source": [
        "## Run Example 2 - Depth Pruning by Percentage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "run-example-2",
      "metadata": {
        "id": "run-example-2"
      },
      "outputs": [],
      "source": [
        "print(\"Starting Example 2: Depth Pruning by Percentage\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Clean up memory from previous example\n",
        "cleanup_memory()\n",
        "\n",
        "# Reload model for second example (since first one was modified)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    torch_dtype=torch.float16 if device.type == 'cuda' else torch.float32,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "# Run Example 2\n",
        "pruned_model_2, stats_2 = example_depth_pruning_by_percentage(model, tokenizer, model_name)\n",
        "\n",
        "# Add to results\n",
        "results.append({\n",
        "    'model': model_name,\n",
        "    'method': 'Percentage',\n",
        "    'param_reduction': stats_2['parameter_reduction_pct'],\n",
        "    'layer_reduction': stats_2['layer_reduction'],\n",
        "    'layers_removed': f\"{stats_2['layer_reduction']}/{stats_2['original_layers']}\"\n",
        "})\n",
        "\n",
        "print(f\"\\nExample 2 completed! Parameter reduction: {stats_2['parameter_reduction_pct']:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "run-example3",
      "metadata": {
        "id": "run-example3"
      },
      "source": [
        "## Run Example 3 - Custom Layer Selection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "run-example-3",
      "metadata": {
        "id": "run-example-3"
      },
      "outputs": [],
      "source": [
        "print(\"Starting Example 3: Custom Layer Selection\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Clean up memory from previous example\n",
        "cleanup_memory()\n",
        "\n",
        "# Reload model for third example (since previous ones were modified)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    torch_dtype=torch.float16 if device.type == 'cuda' else torch.float32,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "# Run Example 3\n",
        "pruned_model_3, stats_3 = example_depth_pruning_custom_layers(model, tokenizer, model_name)\n",
        "\n",
        "if stats_3 is not None:\n",
        "    # Add to results\n",
        "    results.append({\n",
        "        'model': model_name,\n",
        "        'method': 'Custom Layers',\n",
        "        'param_reduction': stats_3['parameter_reduction_pct'],\n",
        "        'layer_reduction': stats_3['layer_reduction'],\n",
        "        'layers_removed': f\"{stats_3['layer_reduction']}/{stats_3['original_layers']}\",\n",
        "        'custom_indices': stats_3['removed_indices']\n",
        "    })\n",
        "    \n",
        "    print(f\"\\nExample 3 completed! Parameter reduction: {stats_3['parameter_reduction_pct']:.2f}%\")\n",
        "else:\n",
        "    print(\"\\nExample 3 skipped due to invalid layer indices.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "summary",
      "metadata": {
        "id": "summary"
      },
      "source": [
        "## Results Summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "results-summary",
      "metadata": {
        "id": "results-summary"
      },
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"DEPTH PRUNING RESULTS SUMMARY\")\n",
        "print(\"=\"*70)\n",
        "print(f\"{'Model':<30} {'Method':<15} {'Param Reduction':<15} {'Layers Removed':<15}\")\n",
        "print(\"-\" * 70)\n",
        "\n",
        "for result in results:\n",
        "    print(f\"{result['model']:<30} {result['method']:<15} {result['param_reduction']:<15.2f}% {result['layers_removed']:<15}\")\n",
        "    if 'custom_indices' in result:\n",
        "        print(f\"{'':>30} {'':>15} Custom indices: {result['custom_indices']}\")\n",
        "\n",
        "print(f\"\\nTotal examples tested: {len(results)}\")\n",
        "print(\"Depth pruning examples completed successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "success",
      "metadata": {
        "id": "success"
      },
      "source": [
        "---\n",
        "## ✅ Success! What's Next?\n",
        "\n",
        "Congratulations! You've successfully performed depth pruning on a transformer model and seen how OptiPFair makes layer removal simple and configurable.\n",
        "\n",
        "**Key takeaways from depth pruning:**\n",
        "- Depth pruning removes entire layers, leading to significant parameter reduction\n",
        "- The \"last\" layer selection method is generally recommended for maintaining performance\n",
        "- Custom layer selection gives you fine-grained control over which layers to remove\n",
        "- Percentage-based pruning allows for consistent reduction ratios across different models\n",
        "\n",
        "If you found this notebook useful, the best way to support the OptiPFair project is by **starring it on GitHub**. Your support is a huge help in boosting the project's visibility and reaching more developers and researchers.\n",
        "\n",
        "### ➡️ [**Star OptiPFair on GitHub**](https://github.com/peremartra/optipfair)\n",
        "\n",
        "---\n",
        "You can also follow my work and new projects on:\n",
        "\n",
        "* **[LinkedIn](https://www.linkedin.com/in/pere-martra/)**\n",
        "* **[X / Twitter](https://twitter.com/PereMartra)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "empty",
      "metadata": {
        "id": "empty"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}