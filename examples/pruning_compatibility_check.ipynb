{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyP0Carl7Y+oSe3zNssuAGx7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/peremartra/optipfair/blob/main/examples/pruning_compatibility_check.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#OptiPFair Notebook Series â€“  Pruning Compatibility Checker\n",
        "\n",
        "![optiPfair Logo](https://github.com/peremartra/optipfair/blob/main/images/optiPfair.png?raw=true)\n",
        "\n",
        "\n",
        "Verify if your model is compatible with [OptiPFair](https://github.com/peremartra/optipfair) MLP pruning capabilities for structured pruning of transformer models with GLU-based MLP layers.  \n",
        "\n",
        "This notebook quickly verifies if your transformer model is compatible with OptipFair's **structured pruning** capabilities.\n",
        "\n",
        "**In 30 seconds, you'll know:**\n",
        "- Can I prune this model with OptipFair?\n",
        "- What's the model architecture?\n",
        "- What are the MLP expansion ratios?\n",
        "- Any specific recommendations?\n",
        "\n",
        "**Supported architectures:** Llama, Mistral, Gemma, Qwen, Phi, and other GLU-based models.\n",
        "\n",
        "##Recommended Environment\n",
        "\n",
        "- **Platform**: [Google Colab](https://colab.research.google.com)  \n",
        "- **Hardware**: GPU runtime (recommended: T4 or better for 1Bâ€“3B models)  \n",
        "- **Dependencies**: Installed automatically in the first cell (optipfair, transformers, torch)\n",
        "\n",
        "##by Pere Martra.\n",
        "\n",
        "- [LinkedIn](https://www.linkedin.com/in/pere-martra)  \n",
        "- [GitHub](https://github.com/peremartra)  \n",
        "- [X / Twitter](https://x.com/peremartra)\n"
      ],
      "metadata": {
        "id": "fAJBzfQOTj2J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup"
      ],
      "metadata": {
        "id": "Cc3wzmUGUnSE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install OptipFair if not already installed\n",
        "!pip install transformers torch -q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jBtixpRoUuBL",
        "outputId": "b329559c-6d22-42e6-98c3-b5d27317789f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m41.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m32.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m29.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m99.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModel, AutoConfig\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"âœ… Setup complete!\")\n",
        "print(f\"ğŸ”¥ PyTorch version: {torch.__version__}\")\n",
        "print(f\"ğŸ¤— Device: {'GPU' if torch.cuda.is_available() else 'CPU'}\")\n",
        "print(\"ğŸ“¦ OptipFair will be used for actual pruning operations\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5VPvLRvYUzC_",
        "outputId": "da2a8b6b-9ac5-42ed-f07e-8ecf18fb5f29"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Setup complete!\n",
            "ğŸ”¥ PyTorch version: 2.6.0+cu124\n",
            "ğŸ¤— Device: GPU\n",
            "ğŸ“¦ OptipFair will be used for actual pruning operations\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Input\n",
        "**Enter your model name below:**  \n",
        "You can use any Hugging Face model ID (e.g., `microsoft/Phi-3-mini-4k-instruct`, `google/gemma-2-2b`)\n",
        "\n",
        "Tested with: Qwen/Qwen3-0.6B, meta-llama/Llama-3.2-1B, google/gemma-3-1b-pt, google/gemma-3-270m\n"
      ],
      "metadata": {
        "id": "p2dRv7SVVtSc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ğŸ‘‡ EDIT THIS: Enter your model name\n",
        "MODEL_NAME = \"google/gemma-3-270m\"  # Change this to test your model\n",
        "#MODEL_NAME = \"meta-llama/Llama-3.2-1B\"  # Change this to test your model\n",
        "#MODEL_NAME = \"Qwen/Qwen3-0.6B\"\n",
        "print(f\"ğŸ” Checking compatibility for: {MODEL_NAME}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2vrWQeLKV0RE",
        "outputId": "698490e3-4a38-4157-95aa-33e142b5e7fd"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ” Checking compatibility for: google/gemma-3-270m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Compatibility Analysis"
      ],
      "metadata": {
        "id": "q7GSN8JwV3Xr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def check_model_compatibility(model_name):\n",
        "    \"\"\"\n",
        "    Comprehensive compatibility check for OptipFair pruning\n",
        "    \"\"\"\n",
        "    try:\n",
        "        print(\"ğŸ”„ Loading model configuration...\")\n",
        "        config = AutoConfig.from_pretrained(model_name)\n",
        "\n",
        "        # Initialize results\n",
        "        results = {\n",
        "            \"model_name\": model_name,\n",
        "            \"compatible\": False,\n",
        "            \"architecture\": \"Unknown\",\n",
        "            \"issues\": [],\n",
        "            \"recommendations\": [],\n",
        "            \"details\": {}\n",
        "        }\n",
        "\n",
        "        # Extract basic info with proper handling of missing fields\n",
        "        results[\"details\"][\"model_type\"] = getattr(config, 'model_type', 'Unknown')\n",
        "        results[\"details\"][\"num_layers\"] = getattr(config, 'num_hidden_layers', 'N/A')\n",
        "        results[\"details\"][\"hidden_size\"] = getattr(config, 'hidden_size', 'N/A')\n",
        "        results[\"details\"][\"intermediate_size\"] = getattr(config, 'intermediate_size', 'N/A')\n",
        "\n",
        "        # Calculate expansion ratio from config if possible\n",
        "        hidden_size = getattr(config, 'hidden_size', None)\n",
        "        intermediate_size = getattr(config, 'intermediate_size', None)\n",
        "\n",
        "        if hidden_size and intermediate_size and hidden_size > 0:\n",
        "            config_expansion_ratio = (intermediate_size / hidden_size) * 100\n",
        "            results[\"details\"][\"config_expansion_ratio\"] = f\"{config_expansion_ratio:.0f}%\"\n",
        "        else:\n",
        "            results[\"details\"][\"config_expansion_ratio\"] = \"N/A\"\n",
        "\n",
        "        print(f\"ğŸ“Š Model type: {results['details']['model_type']}\")\n",
        "        print(f\"ğŸ“Š Layers: {results['details']['num_layers']}\")\n",
        "        print(f\"ğŸ“Š Hidden size: {results['details']['hidden_size']}\")\n",
        "        print(f\"ğŸ“Š Intermediate size: {results['details']['intermediate_size']}\")\n",
        "        print(f\"ğŸ“Š Config expansion ratio: {results['details']['config_expansion_ratio']}\")\n",
        "\n",
        "        return results\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Error loading model: {str(e)}\")\n",
        "        return None"
      ],
      "metadata": {
        "id": "FrPnIXMSWbkY"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the check\n",
        "compatibility_results = check_model_compatibility(MODEL_NAME)"
      ],
      "metadata": {
        "id": "RFaqi8NSWgCu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def analyze_mlp_structure(model_name, config):\n",
        "    \"\"\"\n",
        "    Analyze MLP structure for pruning compatibility\n",
        "    \"\"\"\n",
        "    try:\n",
        "        print(\"\\nğŸ” Analyzing MLP structure...\")\n",
        "\n",
        "        # Load a small portion of the model to inspect structure\n",
        "        model = AutoModel.from_pretrained(\n",
        "            model_name,\n",
        "            torch_dtype=torch.float16,\n",
        "            device_map=\"auto\" if torch.cuda.is_available() else \"cpu\",\n",
        "            trust_remote_code=True\n",
        "        )\n",
        "\n",
        "        # Try to find the first transformer layer (different models have different structures)\n",
        "        first_layer = None\n",
        "        if hasattr(model, 'layers') and len(model.layers) > 0:\n",
        "            first_layer = model.layers[0]\n",
        "        elif hasattr(model, 'transformer') and hasattr(model.transformer, 'h') and len(model.transformer.h) > 0:\n",
        "            first_layer = model.transformer.h[0]\n",
        "        elif hasattr(model, 'model') and hasattr(model.model, 'layers') and len(model.model.layers) > 0:\n",
        "            first_layer = model.model.layers[0]\n",
        "\n",
        "        if first_layer is None:\n",
        "            print(\"âš ï¸  Could not find transformer layers\")\n",
        "            return None\n",
        "\n",
        "        # Try to find MLP/feed_forward layer\n",
        "        mlp = None\n",
        "        if hasattr(first_layer, 'mlp'):\n",
        "            mlp = first_layer.mlp\n",
        "        elif hasattr(first_layer, 'feed_forward'):\n",
        "            mlp = first_layer.feed_forward\n",
        "        elif hasattr(first_layer, 'ffn'):\n",
        "            mlp = first_layer.ffn\n",
        "\n",
        "        if mlp is None:\n",
        "            print(\"âš ï¸  Could not find MLP layer\")\n",
        "            return None\n",
        "\n",
        "        # Check for GLU structure (gate_proj + up_proj + down_proj)\n",
        "        has_gate_proj = hasattr(mlp, 'gate_proj')\n",
        "        has_up_proj = hasattr(mlp, 'up_proj')\n",
        "        has_down_proj = hasattr(mlp, 'down_proj')\n",
        "\n",
        "        # Alternative names for some models\n",
        "        if not has_gate_proj:\n",
        "            has_gate_proj = hasattr(mlp, 'w1') or hasattr(mlp, 'gate_linear')\n",
        "        if not has_up_proj:\n",
        "            has_up_proj = hasattr(mlp, 'w3') or hasattr(mlp, 'up_linear')\n",
        "        if not has_down_proj:\n",
        "            has_down_proj = hasattr(mlp, 'w2') or hasattr(mlp, 'down_linear')\n",
        "\n",
        "        # Calculate expansion ratio\n",
        "        expansion_ratio = 0\n",
        "        input_dim = 0\n",
        "        output_dim = 0\n",
        "\n",
        "        try:\n",
        "            if has_gate_proj and has_up_proj:\n",
        "                gate_layer = getattr(mlp, 'gate_proj', getattr(mlp, 'w1', None))\n",
        "                if gate_layer and hasattr(gate_layer, 'in_features') and hasattr(gate_layer, 'out_features'):\n",
        "                    input_dim = gate_layer.in_features\n",
        "                    output_dim = gate_layer.out_features\n",
        "                    expansion_ratio = (output_dim / input_dim) * 100\n",
        "        except Exception as e:\n",
        "            print(f\"âš ï¸  Could not calculate expansion ratio: {str(e)}\")\n",
        "\n",
        "        # Clean up memory\n",
        "        del model\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "        return {\n",
        "            \"has_glu\": has_gate_proj and has_up_proj and has_down_proj,\n",
        "            \"expansion_ratio\": expansion_ratio,\n",
        "            \"input_dim\": input_dim,\n",
        "            \"output_dim\": output_dim,\n",
        "            \"found_mlp\": True\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"âš ï¸  Could not analyze MLP structure: {str(e)}\")\n",
        "        return {\n",
        "            \"has_glu\": False,\n",
        "            \"expansion_ratio\": 0,\n",
        "            \"input_dim\": 0,\n",
        "            \"output_dim\": 0,\n",
        "            \"found_mlp\": False,\n",
        "            \"error\": str(e)\n",
        "        }\n",
        "\n",
        "# Analyze MLP structure\n",
        "if compatibility_results:\n",
        "    mlp_analysis = analyze_mlp_structure(MODEL_NAME, AutoConfig.from_pretrained(MODEL_NAME))"
      ],
      "metadata": {
        "id": "XT35T3x5WjIW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_final_assessment(results, mlp_analysis):\n",
        "    \"\"\"\n",
        "    Generate final compatibility assessment\n",
        "    \"\"\"\n",
        "    if not results:\n",
        "        print(\"âŒ INCOMPATIBLE: Could not load model configuration\")\n",
        "        return False\n",
        "\n",
        "    if not mlp_analysis or not mlp_analysis.get(\"found_mlp\", False):\n",
        "        print(\"âŒ INCOMPATIBLE: Could not analyze MLP structure\")\n",
        "        results[\"issues\"].append(\"âŒ Could not access MLP layers\")\n",
        "        results[\"compatible\"] = False\n",
        "        return results\n",
        "\n",
        "    # Update results with MLP analysis\n",
        "    results[\"details\"].update(mlp_analysis)\n",
        "\n",
        "    # Determine compatibility\n",
        "    model_type = results[\"details\"][\"model_type\"].lower()\n",
        "    has_glu = mlp_analysis[\"has_glu\"]\n",
        "    expansion_ratio = mlp_analysis[\"expansion_ratio\"]\n",
        "\n",
        "    # Known compatible architectures\n",
        "    compatible_types = [\"llama\", \"mistral\", \"gemma\", \"qwen\", \"phi\"]\n",
        "\n",
        "    if any(comp_type in model_type for comp_type in compatible_types):\n",
        "        results[\"architecture\"] = \"Supported\"\n",
        "        if has_glu and expansion_ratio > 100:\n",
        "            results[\"compatible\"] = True\n",
        "            results[\"recommendations\"].append(f\"âœ… Perfect! GLU structure detected with {expansion_ratio:.0f}% expansion\")\n",
        "        else:\n",
        "            results[\"issues\"].append(\"âŒ No GLU structure found or insufficient expansion\")\n",
        "            if expansion_ratio > 0:\n",
        "                results[\"issues\"].append(f\"âš ï¸  Expansion ratio: {expansion_ratio:.0f}% (minimum 100% recommended)\")\n",
        "    else:\n",
        "        results[\"architecture\"] = \"Unknown/Unsupported\"\n",
        "        results[\"issues\"].append(f\"âŒ Architecture '{model_type}' not yet supported\")\n",
        "        results[\"recommendations\"].append(\"ğŸ“§ Request support via GitHub issues\")\n",
        "\n",
        "    return results\n",
        "\n",
        "# Generate final assessment\n",
        "final_results = generate_final_assessment(compatibility_results, mlp_analysis)"
      ],
      "metadata": {
        "id": "YtG-A_Q7WvHL"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Final Results"
      ],
      "metadata": {
        "id": "qFQ19R0IW02z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def display_results(results):\n",
        "    \"\"\"\n",
        "    Display the final compatibility results in a clean format\n",
        "    \"\"\"\n",
        "    print(\"=\" * 60)\n",
        "    print(\"ğŸ¯ OPTIPFAIR PRUNING COMPATIBILITY REPORT\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # Header\n",
        "    status_emoji = \"âœ…\" if results[\"compatible\"] else \"âŒ\"\n",
        "    status_text = \"COMPATIBLE\" if results[\"compatible\"] else \"NOT COMPATIBLE\"\n",
        "\n",
        "    print(f\"\\n{status_emoji} STATUS: {status_text}\")\n",
        "    print(f\"ğŸ—ï¸  ARCHITECTURE: {results['architecture']}\")\n",
        "    print(f\"ğŸ“¦ MODEL: {results['model_name']}\")\n",
        "\n",
        "    # Details\n",
        "    print(f\"\\nğŸ“Š TECHNICAL DETAILS:\")\n",
        "    details = results[\"details\"]\n",
        "    print(f\"   â€¢ Model Type: {details.get('model_type', 'unknown')}\")\n",
        "    print(f\"   â€¢ Layers: {details.get('num_layers', 'unknown')}\")\n",
        "    print(f\"   â€¢ Hidden Size: {details.get('hidden_size', 'unknown')}\")\n",
        "\n",
        "    if details.get(\"expansion_ratio\", 0) > 0:\n",
        "        print(f\"   â€¢ MLP Expansion: {details['expansion_ratio']:.0f}%\")\n",
        "        print(f\"   â€¢ GLU Structure: {'âœ… Yes' if details.get('has_glu') else 'âŒ No'}\")\n",
        "\n",
        "    # Issues\n",
        "    if results[\"issues\"]:\n",
        "        print(f\"\\nâš ï¸  ISSUES FOUND:\")\n",
        "        for issue in results[\"issues\"]:\n",
        "            print(f\"   {issue}\")\n",
        "\n",
        "    # Recommendations\n",
        "    if results[\"recommendations\"]:\n",
        "        print(f\"\\nğŸ’¡ RECOMMENDATIONS:\")\n",
        "        for rec in results[\"recommendations\"]:\n",
        "            print(f\"   {rec}\")\n",
        "\n",
        "    # Next steps\n",
        "    print(f\"\\nğŸš€ NEXT STEPS:\")\n",
        "    if results[\"compatible\"]:\n",
        "        print(\"   ğŸ“¦ Install OptipFair: pip install optipfair\")\n",
        "        print(\"   ğŸ“ Check the examples/ folder in OptipFair repository\")\n",
        "        print(\"   ğŸ”— https://github.com/peremartra/optipfair\")\n",
        "    else:\n",
        "        print(\"   ğŸ“§ Open an issue: https://github.com/peremartra/optipfair/issues\")\n",
        "        print(\"   ğŸ“š Check supported models: https://github.com/peremartra/optipfair#supported-models\")\n",
        "\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "# Display final results\n",
        "if final_results:\n",
        "    display_results(final_results)\n",
        "else:\n",
        "    print(\"âŒ Could not complete compatibility check\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2INVBFCZW8v-",
        "outputId": "4db45a95-49a4-4a38-9039-318cd9550114"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "ğŸ¯ OPTIPFAIR PRUNING COMPATIBILITY REPORT\n",
            "============================================================\n",
            "\n",
            "âœ… STATUS: COMPATIBLE\n",
            "ğŸ—ï¸  ARCHITECTURE: Supported\n",
            "ğŸ“¦ MODEL: google/gemma-3-270m\n",
            "\n",
            "ğŸ“Š TECHNICAL DETAILS:\n",
            "   â€¢ Model Type: gemma3_text\n",
            "   â€¢ Layers: 18\n",
            "   â€¢ Hidden Size: 640\n",
            "   â€¢ MLP Expansion: 320%\n",
            "   â€¢ GLU Structure: âœ… Yes\n",
            "\n",
            "ğŸ’¡ RECOMMENDATIONS:\n",
            "   âœ… Perfect! GLU structure detected with 320% expansion\n",
            "   âœ… Perfect! GLU structure detected with 320% expansion\n",
            "\n",
            "ğŸš€ NEXT STEPS:\n",
            "   ğŸ“¦ Install OptipFair: pip install optipfair\n",
            "   ğŸ“ Check the examples/ folder in OptipFair repository\n",
            "   ğŸ”— https://github.com/peremartra/optipfair\n",
            "============================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ğŸ”— What's Next?\n",
        "\n",
        "### âœ… If your model is compatible:\n",
        "- **Try pruning:** Run the `basic_pruning_mlp.ipynb` notebook  \n",
        "- **Optimize further:** Experiment with different pruning percentages\n",
        "- **Visualize:** Check out `visualization_compatibility_check.ipynb`\n",
        "\n",
        "### âŒ If your model is not compatible:\n",
        "- **Request support:** Open an issue on [GitHub](https://github.com/peremartra/optipfair/issues)\n",
        "- **Check updates:** New architectures are added regularly\n",
        "- **Contribute:** Help us add support for your model!\n",
        "\n",
        "---\n",
        "\n",
        "## ğŸ“š Learn More\n",
        "\n",
        "- **ğŸ“– Documentation:** [OptipFair GitHub](https://github.com/peremartra/optipfair)  \n",
        "- **ğŸ“ Tutorials:** [Large Language Models Course](https://github.com/peremartra/Large-Language-Model-Notebooks-Course)\n",
        "- **ğŸ¯ Research:** [GLU Expansion Ratios Paper](https://osf.io/preprints/osf/qgxea)\n",
        "\n",
        "---\n",
        "\n",
        "If you found this notebook useful, the best way to support the OptiPFair project is by **starring it on GitHub**. Your support is a huge help in boosting the project's visibility and reaching more developers and researchers.\n",
        "\n",
        "### â¡ï¸ [**Star OptiPFair on GitHub**](https://github.com/peremartra/optipfair)\n",
        "\n",
        "---\n",
        "You can also follow my work and new projects on:\n",
        "\n",
        "* **[LinkedIn](https://www.linkedin.com/in/pere-martra/)**\n",
        "* **[X / Twitter](https://twitter.com/PereMartra)**"
      ],
      "metadata": {
        "id": "XIvVZY5HXCtk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yWVm9C7lTbtg"
      },
      "outputs": [],
      "source": []
    }
  ]
}